from __future__ import annotations

import re
import time
from pathlib import Path
from typing import Dict, List, Tuple, Iterable
from urllib.parse import urlparse

import boto3
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import AmazonTextractPDFLoader
from langchain_aws import BedrockEmbeddings

# DOCX
try:
    import docx as docx_lib  # python-docx
    _HAS_DOCX = True
except Exception:
    _HAS_DOCX = False

# Excel
try:
    import openpyxl
    from openpyxl.utils import get_column_letter
    _HAS_OPENPYXL = True
except Exception:
    _HAS_OPENPYXL = False


# ========================= CONFIG =========================
REGION_S3 = "us-east-1"
REGION_BEDROCK = "us-east-1"

EMBED_MODEL = "amazon.titan-embed-text-v2:0"

# Local root:
# greenlight/gl_to13/<Company>/<Client>/files...
LOCAL_ROOT = Path("greenlight/gl_to13").resolve()

# S3 root mirrors LOCAL_ROOT relative paths:
# s3://bucket/ALL_PDFS/<Company>/<Client>/file.pdf
S3_ROOT_URI = "s3://YOUR_BUCKET/ALL_PDFS/"

# Output root:
# FAISS_INDEX_DIR/<Company>/<Client>/
FAISS_INDEX_DIR = Path("/ABS/PATH/TO/faiss_indices").resolve()

# Group by folder depth:
# 1 => <Company>
# 2 => <Company>/<Client>   ✅ typical company/client layout
CLIENT_GROUP_DEPTH = 2

# Chunking (characters)
CHUNK_SIZE = 1200
CHUNK_OVERLAP = 150

# Excel chunking (rows)
EXCEL_ROWS_PER_CHUNK = 40
EXCEL_MAX_COLS = 80
EXCEL_INCLUDE_EMPTY_COLS = False

# Optional mapping for case mismatch ONLY on the FIRST folder under LOCAL_ROOT.
# Example: {"kpmg":"KPMG"} if S3 has KPMG but local has kpmg
CLIENT_FOLDER_MAP: Dict[str, str] = {}
# ==========================================================


# ========================= TWO SESSIONS (Hardcoded) =========================
# --- TEMP HARDCODE (remove before commit) ---
S3_SESSION = boto3.Session(
    aws_access_key_id="AKIA_S3_ONLY...",
    aws_secret_access_key="S3_SECRET...",
    aws_session_token=None,          # put token if STS
    region_name=REGION_S3,
)

BEDROCK_SESSION = boto3.Session(
    aws_access_key_id="AKIA_BEDROCK_ONLY...",
    aws_secret_access_key="BR_SECRET...",
    aws_session_token=None,          # put token if STS
    region_name=REGION_BEDROCK,
)
# --------------------------------------------------------------------------


# ---------------------- Utilities -------------------------

def clean_text(t: str) -> str:
    t = t or ""
    t = re.sub(r"[ \t]+\n", "\n", t)
    return t.strip()


def chunk_text(text: str, chunk_size: int, overlap: int) -> List[Tuple[int, int, str]]:
    text = text or ""
    n = len(text)
    if n <= chunk_size:
        return [(0, n, text)]
    out: List[Tuple[int, int, str]] = []
    start = 0
    while start < n:
        end = min(n, start + chunk_size)
        out.append((start, end, text[start:end]))
        if end == n:
            break
        start = max(0, end - overlap)
    return out


def parse_s3_uri(s3_uri: str) -> tuple[str, str]:
    if not s3_uri.startswith("s3://"):
        raise ValueError(f"Invalid S3 URI: {s3_uri}")
    p = urlparse(s3_uri)
    return p.netloc, p.path.lstrip("/")


def ensure_trailing_slash(prefix: str) -> str:
    return prefix if (not prefix or prefix.endswith("/")) else prefix + "/"


def _apply_first_folder_mapping(rel_path_posix: str) -> str:
    """
    rel_path_posix like: "<Company>/<Client>/file.pdf"
    Map only the FIRST folder if needed (case mismatch fix).
    """
    parts = rel_path_posix.split("/", 1)
    if not parts:
        return rel_path_posix
    first = parts[0]
    rest = parts[1] if len(parts) > 1 else ""

    mapped = CLIENT_FOLDER_MAP.get(first) or CLIENT_FOLDER_MAP.get(first.lower())
    if mapped:
        return f"{mapped}/{rest}" if rest else mapped
    return rel_path_posix


def s3_uri_from_local_path(local_file: Path) -> str:
    """
    Maps local file -> S3 by preserving relative path under LOCAL_ROOT.
    """
    bucket, s3_prefix = parse_s3_uri(S3_ROOT_URI)
    s3_prefix = ensure_trailing_slash(s3_prefix)

    rel = local_file.relative_to(LOCAL_ROOT).as_posix()   # preserves case
    rel = _apply_first_folder_mapping(rel)                # optional
    return f"s3://{bucket}/{s3_prefix}{rel}"


def group_key_for_file(local_file: Path, depth: int) -> Path:
    """
    Example local relative path:
      Company/Client/sub/file.pdf
    depth=2 -> Company/Client
    depth=1 -> Company
    """
    rel = local_file.relative_to(LOCAL_ROOT)
    parent_parts = rel.parts[:-1]  # directories only
    if not parent_parts:
        return Path("_ungrouped")
    depth = max(1, depth)
    use_parts = parent_parts[: min(depth, len(parent_parts))]
    return Path(*use_parts)


# ------------------- Local Scan (group by depth) ---------------------------

PDF_EXTS = {".pdf"}
DOCX_EXTS = {".docx"}
EXCEL_EXTS = {".xlsx", ".xlsm"}
SUPPORTED_EXTS = PDF_EXTS | DOCX_EXTS | EXCEL_EXTS


def scan_local_files_grouped(root: Path, depth: int) -> Dict[Path, List[Path]]:
    grouped: Dict[Path, List[Path]] = {}
    for f in root.rglob("*"):
        if not f.is_file():
            continue
        if f.suffix.lower() not in SUPPORTED_EXTS:
            continue
        key = group_key_for_file(f, depth)
        grouped.setdefault(key, []).append(f)

    for k in grouped:
        grouped[k] = sorted(grouped[k], key=lambda p: p.name)
    return dict(sorted(grouped.items(), key=lambda kv: str(kv[0])))


# ---------------- PDF extraction (Textract for ALL PDFs) ----------------

def extract_textract_pdf_chunks(s3_pdf_uri: str, group_id: str, local_pdf: Path) -> Iterable[Document]:
    """
    Textract extraction for ALL PDFs.
    Uses S3_SESSION credentials by passing an explicit textract client.
    """
    filename = local_pdf.name

    textract_client = S3_SESSION.client("textract", region_name=REGION_S3)
    loader = AmazonTextractPDFLoader(file_path=s3_pdf_uri, client=textract_client)

    docs = loader.load()

    for d in docs:
        page_text = clean_text(d.page_content or "")
        if not page_text:
            continue

        page_num = None
        if isinstance(d.metadata, dict):
            page_num = d.metadata.get("page") or d.metadata.get("Page")

        for _, _, chunk in chunk_text(page_text, CHUNK_SIZE, CHUNK_OVERLAP):
            meta = {
                "group_id": group_id,
                "file_type": "pdf",
                "source_name": filename,
                "source_file": s3_pdf_uri,
                "local_file": str(local_pdf),
                "extracted_by": "textract",
            }
            if page_num is not None:
                try:
                    meta["page"] = int(page_num)
                except Exception:
                    pass
            yield Document(page_content=chunk, metadata=meta)


# ---------------- DOCX extraction ----------------

def _docx_table_to_text(table) -> str:
    rows = []
    for r in table.rows:
        cells = [clean_text(c.text) for c in r.cells]
        if any(cells):
            rows.append(" | ".join(cells))
    return "\n".join(rows).strip()


def extract_docx_chunks(local_docx: Path, group_id: str) -> Iterable[Document]:
    if not _HAS_DOCX:
        raise RuntimeError("python-docx not installed. Install it to index .docx files.")

    filename = local_docx.name
    doc = docx_lib.Document(str(local_docx))

    blocks: List[str] = []
    for p in doc.paragraphs:
        t = clean_text(p.text)
        if t:
            blocks.append(t)

    for ti, tbl in enumerate(doc.tables):
        tt = _docx_table_to_text(tbl)
        if tt:
            blocks.append(f"[TABLE {ti+1}]\n{tt}")

    full_text = clean_text("\n\n".join(blocks))
    if not full_text:
        return

    for _, _, chunk in chunk_text(full_text, CHUNK_SIZE, CHUNK_OVERLAP):
        yield Document(
            page_content=chunk,
            metadata={
                "group_id": group_id,
                "file_type": "docx",
                "source_name": filename,
                "local_file": str(local_docx),
                "extracted_by": "python-docx",
            },
        )


# ---------------- Excel extraction (all sheets, context preserved) ----------------

def _cell_to_str(v) -> str:
    if v is None:
        return ""
    try:
        return str(v).strip()
    except Exception:
        return ""


def _find_header_row(rows: List[List[str]]) -> Tuple[int, List[str]]:
    for idx, r in enumerate(rows):
        if any(c.strip() for c in r):
            headers = r[:]
            for j in range(len(headers)):
                if not headers[j].strip():
                    headers[j] = get_column_letter(j + 1)
            return idx, headers
    return 0, []


def extract_excel_workbook_chunks(excel_path: Path, group_id: str) -> Iterable[Document]:
    if not _HAS_OPENPYXL:
        raise RuntimeError("openpyxl not installed. Install it to index .xlsx/.xlsm files.")

    filename = excel_path.name
    wb = openpyxl.load_workbook(filename=str(excel_path), data_only=True, read_only=True)

    try:
        for ws in wb.worksheets:
            raw_rows: List[List[str]] = []
            for row in ws.iter_rows(values_only=True):
                r = [_cell_to_str(v) for v in (row[:EXCEL_MAX_COLS] if row else [])]
                raw_rows.append(r)

            if not raw_rows:
                continue

            header_idx, headers = _find_header_row(raw_rows)
            if not headers:
                continue

            data_start = header_idx + 1
            data_rows = raw_rows[data_start:]

            row_lines: List[Tuple[int, str]] = []
            excel_row_num = data_start + 1  # 1-based

            for r in data_rows:
                r = (r + [""] * len(headers))[:len(headers)]
                if not any(c.strip() for c in r):
                    excel_row_num += 1
                    continue

                parts = []
                for h, v in zip(headers, r):
                    if v.strip() or EXCEL_INCLUDE_EMPTY_COLS:
                        parts.append(f"{h}={v}")
                if parts:
                    row_lines.append((excel_row_num, " | ".join(parts)))
                excel_row_num += 1

            if not row_lines:
                continue

            sheet_name = ws.title
            header_line = "Headers: " + " | ".join(headers)

            i = 0
            while i < len(row_lines):
                chunk_rows = row_lines[i:i + EXCEL_ROWS_PER_CHUNK]
                row_start = chunk_rows[0][0]
                row_end = chunk_rows[-1][0]
                body_lines = [f"Row {rn} | {txt}" for rn, txt in chunk_rows]

                content = clean_text("\n".join([
                    f"Workbook: {filename}",
                    f"Sheet: {sheet_name}",
                    header_line,
                    f"Rows: {row_start}-{row_end}",
                    "",
                    *body_lines,
                ]))

                yield Document(
                    page_content=content,
                    metadata={
                        "group_id": group_id,
                        "file_type": "excel",
                        "source_name": filename,
                        "sheet": sheet_name,
                        "row_start": row_start,
                        "row_end": row_end,
                        "local_file": str(excel_path),
                        "extracted_by": "openpyxl",
                    },
                )

                i += EXCEL_ROWS_PER_CHUNK
    finally:
        wb.close()


# ---------------- Embeddings / Indexing -------------------

def get_embeddings() -> BedrockEmbeddings:
    bedrock_runtime = BEDROCK_SESSION.client("bedrock-runtime", region_name=REGION_BEDROCK)
    return BedrockEmbeddings(model_id=EMBED_MODEL, client=bedrock_runtime)


def index_one_group(group_key: Path, files: List[Path], embeddings: BedrockEmbeddings) -> None:
    """
    ✅ Saves FAISS to FAISS_INDEX_DIR/<group_key>/ e.g. <Company>/<Client>/
    """
    out_dir = FAISS_INDEX_DIR / group_key
    out_dir.mkdir(parents=True, exist_ok=True)

    group_id = group_key.as_posix()  # "Company/Client"
    all_docs: List[Document] = []
    t0 = time.time()

    for f in files:
        ext = f.suffix.lower()

        if ext in PDF_EXTS:
            s3_uri = s3_uri_from_local_path(f)
            print(f"[{group_id}] PDF -> Textract: {f.name}")
            print(f"           S3 URI: {s3_uri}")
            try:
                for d in extract_textract_pdf_chunks(s3_uri, group_id, f):
                    all_docs.append(d)
            except Exception as e:
                print(f"[WARN] Textract failed for {s3_uri}: {e}")

        elif ext in DOCX_EXTS:
            print(f"[{group_id}] DOCX -> python-docx: {f.name}")
            for d in extract_docx_chunks(f, group_id):
                all_docs.append(d)

        elif ext in EXCEL_EXTS:
            print(f"[{group_id}] Excel -> openpyxl: {f.name}")
            for d in extract_excel_workbook_chunks(f, group_id):
                all_docs.append(d)

    if not all_docs:
        print(f"[{group_id}] No documents extracted. Skipping.")
        return

    print(f"[{group_id}] Building FAISS with {len(all_docs)} docs...")
    vs = FAISS.from_documents(all_docs, embeddings)
    vs.save_local(str(out_dir))

    print(f"[{group_id}] ✅ Saved FAISS at: {out_dir} (took {time.time() - t0:.1f}s)")


def main():
    if not LOCAL_ROOT.exists():
        raise SystemExit(f"LOCAL_ROOT not found: {LOCAL_ROOT}")

    FAISS_INDEX_DIR.mkdir(parents=True, exist_ok=True)

    grouped = scan_local_files_grouped(LOCAL_ROOT, CLIENT_GROUP_DEPTH)
    if not grouped:
        print("No supported files found under LOCAL_ROOT.")
        return

    embeddings = get_embeddings()

    print(f"Found {sum(len(v) for v in grouped.values())} files across {len(grouped)} group(s).")
    for group_key, files in grouped.items():
        print(f"\n=== Indexing group: {group_key.as_posix()} | files={len(files)} ===")
        index_one_group(group_key, files, embeddings)


if __name__ == "__main__":
    main()
