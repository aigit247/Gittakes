# indexer.py
from __future__ import annotations

import io
import json
import os
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import boto3
import fitz  # PyMuPDF
import pandas as pd
from langchain.schema import Document
from langchain_aws import BedrockEmbeddings
from langchain_community.document_loaders import AmazonTextractPDFLoader
from langchain_community.vectorstores import FAISS


# ===================== LOCAL PATH CONFIG =====================

LOCAL_DOC_ROOT = Path("ce").resolve()

CATEGORY_ROOTS: Dict[str, Path] = {
    "legal": LOCAL_DOC_ROOT / "legal",
    "finance": LOCAL_DOC_ROOT / "finance",
}

FAISS_INDEX_DIR = Path(os.getenv("FAISS_INDEX_DIR_LOCAL_OUT", "/opt/ml/faiss_indices")).resolve()

# ===================== S3 DOC CONFIG (for Textract loader) =====================
S3_DOC_BUCKET = os.getenv("S3_DOC_BUCKET", "").strip()  # required if using Textract loader
S3_DOC_PREFIX = os.getenv("S3_DOC_PREFIX", "").strip()  # maps LOCAL_DOC_ROOT -> S3 prefix, e.g. "user/d/ce"

# ===================== AWS / BEDROCK / TEXTRACT CONFIG =====================

REGION = os.getenv("AWS_REGION", "us-east-1")
EMBED_MODEL = os.getenv("BEDROCK_EMBED_MODEL", "amazon.titan-embed-text-v2:0")

# ===================== Separate Sessions: S3/Textract vs Bedrock (with tokens) =====================
# S3/Textract credentials
S3_AWS_ACCESS_KEY_ID = os.getenv("S3_AWS_ACCESS_KEY_ID") or os.getenv("AWS_ACCESS_KEY_ID")
S3_AWS_SECRET_ACCESS_KEY = os.getenv("S3_AWS_SECRET_ACCESS_KEY") or os.getenv("AWS_SECRET_ACCESS_KEY")
S3_AWS_SESSION_TOKEN = os.getenv("S3_AWS_SESSION_TOKEN") or os.getenv("AWS_SESSION_TOKEN")

# Bedrock credentials
BEDROCK_AWS_ACCESS_KEY_ID = os.getenv("BEDROCK_AWS_ACCESS_KEY_ID")
BEDROCK_AWS_SECRET_ACCESS_KEY = os.getenv("BEDROCK_AWS_SECRET_ACCESS_KEY")
BEDROCK_AWS_SESSION_TOKEN = os.getenv("BEDROCK_AWS_SESSION_TOKEN")


def _mk_session(
    access_key_id: Optional[str],
    secret_access_key: Optional[str],
    session_token: Optional[str],
) -> boto3.Session:
    """
    If access_key_id/secret_access_key provided, build an explicit Session.
    If token is provided (STS creds), include aws_session_token.
    Otherwise fall back to default credential chain.
    """
    if access_key_id and secret_access_key:
        kwargs = {
            "aws_access_key_id": access_key_id,
            "aws_secret_access_key": secret_access_key,
            "region_name": REGION,
        }
        if session_token:
            kwargs["aws_session_token"] = session_token
        return boto3.Session(**kwargs)
    return boto3.Session(region_name=REGION)


S3_SESSION = _mk_session(S3_AWS_ACCESS_KEY_ID, S3_AWS_SECRET_ACCESS_KEY, S3_AWS_SESSION_TOKEN)
BEDROCK_SESSION = _mk_session(BEDROCK_AWS_ACCESS_KEY_ID, BEDROCK_AWS_SECRET_ACCESS_KEY, BEDROCK_AWS_SESSION_TOKEN)

TEXTRACT_CLIENT = S3_SESSION.client("textract")
BEDROCK_RUNTIME = BEDROCK_SESSION.client("bedrock-runtime")

# ===================== XLSX mapping =====================

CLIENT_GROUP_XLSX_PATH = (os.getenv("CLIENT_GROUP_XLSX_PATH") or "").strip()


# ===================== Manifest metadata helpers =====================
def load_metadata_for_file(doc_path: Path) -> Optional[dict]:
    """
    Looks for sidecar metadata files:
      <file>.metadata.json
      <file>.metadata

    Returns:
      - data["metadataAtrributes"] OR data["metadataAttributes"] if present
      - else whole JSON object
      - else None
    """
    meta_candidates = [
        doc_path.with_name(doc_path.name + ".metadata.json"),
        doc_path.with_name(doc_path.name + ".metadata"),
    ]

    for meta_path in meta_candidates:
        if not meta_path.exists():
            continue
        try:
            raw = meta_path.read_text(encoding="utf-8", errors="ignore").strip()
            data = json.loads(raw)
        except Exception:
            continue

        meta_block = data.get("metadataAtrributes") or data.get("metadataAttributes") or data
        return meta_block

    return None


def page_count_for_file(path: Path) -> int:
    if path.suffix.lower() != ".pdf":
        return 0
    try:
        with fitz.open(str(path)) as doc:
            return int(doc.page_count or 0)
    except Exception:
        return 0


# ---------- text helpers ----------
_LEGAL_SUFFIX_TOKENS = {"inc", "ltd", "llc", "plc", "as", "sa", "ag", "gmbh", "bv", "nv", "s"}


def _clean_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()


def canonicalize_name(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9]+", " ", s)
    return _clean_spaces(s)


def canonicalize_compact(s: str) -> str:
    return canonicalize_name(s).replace(" ", "")


def acronym(name: str) -> str:
    toks = canonicalize_name(name).split()
    while toks and toks[-1] in _LEGAL_SUFFIX_TOKENS:
        toks.pop()
    return "".join(t[0] for t in toks).upper() if toks else ""


def normalize_country(country: Optional[str]) -> Optional[str]:
    if not country:
        return None
    c = str(country).strip().upper()
    if c in {"UNITED STATES", "U.S.", "U.S", "US", "USA"}:
        return "US"
    if c in {"UNITED KINGDOM", "GREAT BRITAIN", "GB", "UK"}:
        return "UK"
    if c in {"INDIA", "IN", "IND"}:
        return "IN"
    return c or None


def scope_folder_name(scope_input: str, scope_kind: str, scope_value: Optional[str]) -> str:
    sk = (scope_kind or "").strip().lower()
    if sk == "ultimate_parent":
        return "Ultimate Parent"
    if sk == "corporate_family":
        return "Corporate Family"
    return (scope_value or normalize_country(scope_input) or (scope_input or "")).strip().upper() or "COUNTRY"


# ---------- S3 helpers ----------
def _is_s3_uri(s: str) -> bool:
    return isinstance(s, str) and s.lower().startswith("s3://")


def _parse_s3_uri(uri: str) -> Tuple[str, str]:
    from urllib.parse import urlparse
    u = urlparse((uri or "").strip())
    bucket = (u.netloc or "").strip()
    key = (u.path or "").lstrip("/")
    if not bucket:
        raise ValueError(f"Invalid S3 URI (missing bucket): {uri}")
    return bucket, key


def _list_keys(s3_client, bucket: str, prefix: str) -> List[str]:
    prefix = (prefix or "").lstrip("/")
    if prefix and not prefix.endswith("/"):
        prefix = prefix + "/"
    paginator = s3_client.get_paginator("list_objects_v2")
    out: List[str] = []
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []) or []:
            k = obj.get("Key") or ""
            if k and not k.endswith("/"):
                out.append(k)
    return out


def _resolve_s3_excel_object(s3_client, xlsx_s3_uri: str) -> Tuple[str, str]:
    bucket, key = _parse_s3_uri(xlsx_s3_uri)
    lk = (key or "").lower()
    if lk.endswith(".xlsx") or lk.endswith(".xls"):
        return bucket, key
    keys = _list_keys(s3_client, bucket, key)
    excels = [k for k in keys if k.lower().endswith(".xlsx") or k.lower().endswith(".xls")]
    if not excels:
        raise FileNotFoundError(f"No .xlsx/.xls found under s3://{bucket}/{key.rstrip('/')}/")
    excels.sort()
    return bucket, excels[0]


def _norm_col(c: str) -> str:
    return re.sub(r"[^a-z0-9]+", "", (c or "").lower())


def _canonicalize_xlsx_headers(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [str(c).strip() for c in df.columns]
    norm_to_actual = {_norm_col(c): c for c in df.columns}

    # client column: tolerate 'cleint'
    client_candidates = []
    for nk, actual in norm_to_actual.items():
        if "clientname" in nk or "cleintname" in nk:
            client_candidates.append(actual)
    if client_candidates and "clientName" not in df.columns:
        df = df.rename(columns={client_candidates[0]: "clientName"})

    # group column: any 'group...' header
    if "groupid" not in df.columns:
        group_keys = [nk for nk in norm_to_actual.keys() if nk.startswith("group")]
        if group_keys:
            df = df.rename(columns={norm_to_actual[group_keys[0]]: "groupid"})
    return df


_SCOPE_UP = canonicalize_name("Ultimate Parent")
_SCOPE_CF = canonicalize_name("Corporate Family")


def _split_clientname_pattern(s: str) -> Tuple[str, str]:
    s = _clean_spaces(s)
    for sep in [" - ", " – ", " — "]:
        if sep in s:
            base, suf = s.rsplit(sep, 1)
            return base.strip(), suf.strip()
    return s.strip(), ""


def _scope_kind_and_value(scope_raw: str) -> Tuple[str, Optional[str]]:
    sr = _clean_spaces(scope_raw)
    sc = canonicalize_name(sr)
    if sc == _SCOPE_UP:
        return "ultimate_parent", None
    if sc == _SCOPE_CF:
        return "corporate_family", None
    return "country", normalize_country(sr)


@dataclass(frozen=True)
class MappingRow:
    client_display: str
    client_canon: str
    client_compact: str
    client_acronym: str
    scope_kind: str
    scope_value: Optional[str]
    group_id: str
    scope_dir: str


class ClientGroupMapper:
    def __init__(self, xlsx_path: str):
        self.xlsx_path = str(xlsx_path or "").strip()
        self.rows: List[MappingRow] = []
        self._by_key: Dict[Tuple[str, str, Optional[str]], MappingRow] = {}
        self._display_by_canon: Dict[str, str] = {}
        self._clients_canon: List[str] = []
        self._clients_compact: Dict[str, str] = {}
        self._acr_to_canons: Dict[str, List[str]] = {}

    def load(self, s3_client) -> None:
        if self.rows:
            return
        if not self.xlsx_path:
            raise ValueError("CLIENT_GROUP_XLSX_PATH is required")

        if _is_s3_uri(self.xlsx_path):
            b, k = _resolve_s3_excel_object(s3_client, self.xlsx_path)
            obj = s3_client.get_object(Bucket=b, Key=k)
            data = obj["Body"].read()
        else:
            if not os.path.exists(self.xlsx_path):
                raise FileNotFoundError(f"XLSX not found: {self.xlsx_path}")
            data = Path(self.xlsx_path).read_bytes()

        df = pd.read_excel(io.BytesIO(data), dtype=str, keep_default_na=False)
        df = _canonicalize_xlsx_headers(df)

        if "clientName" not in df.columns or "groupid" not in df.columns:
            raise ValueError(f"XLSX must have clientName/groupid after rename. Got columns={list(df.columns)}")

        for _, r in df.iterrows():
            raw_client = str(r.get("clientName") or "").strip()
            raw_gid = str(r.get("groupid") or "").strip()
            if not raw_client or not raw_gid:
                continue

            base, scope = _split_clientname_pattern(raw_client)
            kind, val = _scope_kind_and_value(scope)

            canon = canonicalize_name(base)
            comp = canonicalize_compact(base)
            acr = acronym(base)
            sdir = scope_folder_name(scope, kind, val)

            row = MappingRow(
                client_display=base.strip(),
                client_canon=canon,
                client_compact=comp,
                client_acronym=acr,
                scope_kind=kind,
                scope_value=val,
                group_id=raw_gid.strip(),
                scope_dir=sdir,
            )

            self.rows.append(row)
            self._by_key[(canon, kind, val)] = row
            self._display_by_canon.setdefault(canon, base.strip())
            self._clients_canon.append(canon)
            self._clients_compact.setdefault(comp, canon)
            if acr:
                self._acr_to_canons.setdefault(acr, [])
                if canon not in self._acr_to_canons[acr]:
                    self._acr_to_canons[acr].append(canon)

        self._clients_canon = sorted(set(self._clients_canon))

    def _resolve_client_canon(self, client_input: str) -> Optional[str]:
        ci = (client_input or "").strip()
        if not ci:
            return None
        c = canonicalize_name(ci)
        cc = canonicalize_compact(ci)
        acrv = ci.strip().upper()

        if c in self._display_by_canon:
            return c
        if cc in self._clients_compact:
            return self._clients_compact[cc]
        hits = self._acr_to_canons.get(acrv) or []
        if len(hits) == 1:
            return hits[0]

        subs = [canon for canon in self._clients_canon if canon and (canon in c or c in canon)]
        if subs:
            subs.sort(key=len, reverse=True)
            return subs[0]

        try:
            from rapidfuzz import process, fuzz  # type: ignore
            best = process.extractOne(c, self._clients_canon, scorer=fuzz.token_set_ratio)
            if best:
                best_key, score, _ = best
                if float(score) >= 80:
                    return best_key
        except Exception:
            pass

        return None

    def resolve_group_row(self, client_input: str, scope_input: str) -> Optional[MappingRow]:
        canon = self._resolve_client_canon(client_input)
        if not canon:
            return None
        kind, val = _scope_kind_and_value(scope_input or "")
        return self._by_key.get((canon, kind, val))


# ===================== Chunking/extraction =====================

CHUNK_SIZE = 1200
CHUNK_OVERLAP = 150


def clean_text(t: str) -> str:
    return re.sub(r"[ \t]+\n", "\n", t or "").strip()


def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[Tuple[int, int, str]]:
    text = text or ""
    n = len(text)
    if n <= chunk_size:
        return [(0, n, text)]
    chunks = []
    start = 0
    while start < n:
        end = min(n, start + chunk_size)
        chunk = text[start:end]
        chunks.append((start, end, chunk))
        if end == n:
            break
        start = max(0, end - overlap)
    return chunks


def local_path_to_s3_key(path: Path) -> Optional[str]:
    if not S3_DOC_BUCKET:
        return None
    try:
        rel = path.resolve().relative_to(LOCAL_DOC_ROOT.resolve())
    except ValueError:
        return None
    rel_key = rel.as_posix()
    if S3_DOC_PREFIX:
        return f"{S3_DOC_PREFIX.rstrip('/')}/{rel_key}"
    return rel_key


def is_scanned_pdf(pdf_path: Path, sample_pages: int = 3, char_threshold: int = 50) -> bool:
    try:
        with fitz.open(str(pdf_path)) as doc:
            n_pages = doc.page_count
            if n_pages == 0:
                return True
            to_sample = min(n_pages, sample_pages)
            total_chars = 0
            for i in range(to_sample):
                page = doc.load_page(i)
                txt = page.get_text("text") or ""
                total_chars += len(txt.strip())
            return total_chars < char_threshold * to_sample
    except Exception:
        return True


def make_chunks_for_pdf(pdf_path: Path, category: str, client_folder: str, group_id: str, scope_dir: str) -> Iterable[Document]:
    policy_id = pdf_path.name

    # If scanned, try Textract loader on S3 URI
    if is_scanned_pdf(pdf_path):
        s3_key = local_path_to_s3_key(pdf_path)
        if s3_key and S3_DOC_BUCKET:
            s3_uri = f"s3://{S3_DOC_BUCKET}/{s3_key}"
            try:
                loader = AmazonTextractPDFLoader(file_path=s3_uri, client=TEXTRACT_CLIENT)
                docs = loader.load()
            except Exception:
                docs = []
            for d in docs or []:
                txt = clean_text(d.page_content or "")
                if not txt:
                    continue
                meta = dict(d.metadata or {})
                page_num = int(meta.get("page") or 1)
                for _, _, chunk in chunk_text(txt):
                    yield Document(
                        page_content=chunk,
                        metadata={
                            "category": category,
                            "client_folder": client_folder,
                            "group_id": group_id,
                            "scope_dir": scope_dir,
                            "policy_id": policy_id,
                            "page_start": page_num,
                            "page_end": page_num,
                            "source_file": str(pdf_path.resolve()),
                            "extracted_by": "amazon_textract_loader",
                        },
                    )
            return

    # Normal PDF text extraction
    try:
        with fitz.open(str(pdf_path)) as doc:
            texts = []
            page_ranges: List[Tuple[int, int, int]] = []
            cursor = 0

            for i in range(doc.page_count):
                page = doc.load_page(i)
                txt = clean_text(page.get_text("text") or "")
                if not txt:
                    continue
                start = cursor
                texts.append(txt + "\n")
                cursor += len(txt) + 1
                page_ranges.append((start, cursor, i + 1))

            full = "".join(texts).strip()
            if not full:
                return

            for c_start, c_end, chunk in chunk_text(full):
                pages = sorted({
                    p for p_start, p_end, p in page_ranges
                    if not (p_end <= c_start or p_start >= c_end)
                })
                if not pages:
                    continue
                yield Document(
                    page_content=chunk,
                    metadata={
                        "category": category,
                        "client_folder": client_folder,
                        "group_id": group_id,
                        "scope_dir": scope_dir,
                        "policy_id": policy_id,
                        "page_start": pages[0],
                        "page_end": pages[-1],
                        "source_file": str(pdf_path.resolve()),
                        "extracted_by": "pymupdf_text",
                    },
                )
    except Exception:
        return


def make_chunks_for_docx(doc_path: Path, category: str, client_folder: str, group_id: str, scope_dir: str) -> Iterable[Document]:
    try:
        from docx import Document as DocxDocument
    except Exception:
        return

    policy_id = doc_path.name
    try:
        doc = DocxDocument(str(doc_path))
    except Exception:
        return

    texts = []
    for para in doc.paragraphs:
        t = (para.text or "").strip()
        if t:
            texts.append(t)
    full = "\n".join(texts).strip()
    if not full:
        return

    for logical_page, (_, _, chunk) in enumerate(chunk_text(full), start=1):
        yield Document(
            page_content=chunk,
            metadata={
                "category": category,
                "client_folder": client_folder,
                "group_id": group_id,
                "scope_dir": scope_dir,
                "policy_id": policy_id,
                "page_start": logical_page,
                "page_end": logical_page,
                "source_file": str(doc_path.resolve()),
                "extracted_by": "python-docx",
            },
        )


def make_chunks_for_file(path: Path, category: str, client_folder: str, group_id: str, scope_dir: str) -> Iterable[Document]:
    suf = path.suffix.lower()
    if suf == ".pdf":
        yield from make_chunks_for_pdf(path, category, client_folder, group_id, scope_dir)
    elif suf in (".docx",):
        yield from make_chunks_for_docx(path, category, client_folder, group_id, scope_dir)


def embedder():
    return BedrockEmbeddings(model_id=EMBED_MODEL, client=BEDROCK_RUNTIME)


# ===================== Scan contract folders =====================

def scan_tasks() -> List[Tuple[str, str, str, List[Path]]]:
    """
    Returns list of tasks:
      (category, client_folder, scope_input, files)
    For finance: ce/finance/<client>/<scope>/
    For legal:   ce/legal/<client>/  (scope_input="")
    """
    tasks: List[Tuple[str, str, str, List[Path]]] = []

    # legal
    legal_root = CATEGORY_ROOTS["legal"]
    if legal_root.exists():
        for client_dir in legal_root.iterdir():
            if not client_dir.is_dir():
                continue
            client_folder = client_dir.name
            files = sorted(
                [*client_dir.rglob("*.pdf"), *client_dir.rglob("*.docx")],
                key=lambda p: p.name.lower(),
            )
            if files:
                tasks.append(("legal", client_folder, "", files))

    # finance
    fin_root = CATEGORY_ROOTS["finance"]
    if fin_root.exists():
        for client_dir in fin_root.iterdir():
            if not client_dir.is_dir():
                continue
            client_folder = client_dir.name
            for scope_dir in client_dir.iterdir():
                if not scope_dir.is_dir():
                    continue
                scope_input = scope_dir.name  # e.g. "US", "Ultimate Parent", "Corporate Family"
                files = sorted(
                    [*scope_dir.rglob("*.pdf"), *scope_dir.rglob("*.docx")],
                    key=lambda p: p.name.lower(),
                )
                if files:
                    tasks.append(("finance", client_folder, scope_input, files))

    return tasks


# ===================== Build index per task =====================

def index_task(mapper: ClientGroupMapper, category: str, client_folder: str, scope_input: str, files: List[Path]) -> None:
    row = mapper.resolve_group_row(client_folder, scope_input)
    if not row:
        print(f"[SKIP] No mapping for client='{client_folder}' scope='{scope_input}' category='{category}'")
        return

    group_id = row.group_id
    scope_dir = row.scope_dir

    out_dir = FAISS_INDEX_DIR / category / group_id / scope_dir
    out_dir.mkdir(parents=True, exist_ok=True)

    docs: List[Document] = []
    manifest_docs: List[dict] = []

    for path in files:
        for d in make_chunks_for_file(path, category, client_folder, group_id, scope_dir):
            docs.append(d)

        manifest_docs.append(
            {
                "policy_id": path.name,
                "source_file": str(path.resolve()),
                "pages": page_count_for_file(path),
                "modified_time": int(path.stat().st_mtime),
                "metadata": load_metadata_for_file(path),
            }
        )

    if not docs:
        print(f"[{category}/{client_folder}/{scope_dir}] No extractable text; skipping.")
        return

    print(f"[{category}/{client_folder}/{scope_dir}] Building FAISS with {len(docs)} chunks...")
    vs = FAISS.from_documents(docs, embedder())
    vs.save_local(str(out_dir))

    manifest = {
        "category": category,
        "client_folder": client_folder,
        "group_id": group_id,
        "scope_input": scope_input,
        "scope_dir": scope_dir,
        "docs": manifest_docs,
        "generated_time": int(time.time()),
    }
    (out_dir / "manifest.json").write_text(json.dumps(manifest, indent=2), encoding="utf-8")
    print(f"[{category}/{client_folder}/{scope_dir}] Done. Index at {out_dir}")


def main():
    if not CLIENT_GROUP_XLSX_PATH:
        raise RuntimeError("CLIENT_GROUP_XLSX_PATH is required")

    FAISS_INDEX_DIR.mkdir(parents=True, exist_ok=True)

    # Use S3_SESSION for reading XLSX from S3 (and any S3 access)
    s3 = S3_SESSION.client("s3")
    mapper = ClientGroupMapper(CLIENT_GROUP_XLSX_PATH)
    mapper.load(s3)

    tasks = scan_tasks()
    if not tasks:
        print("No tasks found.")
        return

    for category, client_folder, scope_input, files in tasks:
        index_task(mapper, category, client_folder, scope_input, files)


if __name__ == "__main__":
    main()
