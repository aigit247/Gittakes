# Baseline (NO FAISS): TEXT-first, then TABLE fallback (whole-table chunks), deterministic.
# UPDATED: Uses S3 prefix exactly like your working code + separate S3/Textract creds vs Bedrock creds,
#          and does s3.head_object() sanity check before Textract.

import os, re, json, time
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import urlparse

import boto3

# Optional deps (install if missing):
#   pip install python-docx openpyxl pypdf
try:
    import docx  # python-docx
except Exception:
    docx = None

try:
    import openpyxl
except Exception:
    openpyxl = None

try:
    from pypdf import PdfReader
except Exception:
    PdfReader = None


# =========================
# CONFIG (EDIT THESE)
# =========================
COMPANY = "company1"
CLIENT  = "client1"

# Local folder: data/<Company>/<Client>/**/*
DATA_ROOT = Path("data")

# IMPORTANT: Your REAL S3 base prefix (matches what you told: s3://bucketname/user/tt/sid/data)
S3_PDF_DIR = "s3://bucketname/user/tt/sid/data"   # <-- EDIT THIS

AWS_REGION = os.environ.get("AWS_REGION", "us-east-1")

# Bedrock Claude model id (set yours)
BEDROCK_MODEL_ID = os.environ.get("BEDROCK_MODEL_ID", "anthropic.claude-3-5-sonnet-20240620-v1:0")

# Determinism
TEMPERATURE = 0

# Chunking
PDF_TEXT_LINES_PER_SNIPPET = 15
DOCX_PARAS_PER_SNIPPET     = 6
MAX_TABLE_ROWS_PER_CHUNK   = 350

# Retrieval caps
K_TEXT_PER_FIELD  = 8
K_TABLE_PER_FIELD = 3
MAX_TEXT_SNIPPETS_TOTAL  = 120
MAX_TABLE_SNIPPETS_TOTAL = 35

# Fields + synonyms (use yours)
FIELDS = [
   
]
FIELD_SYNONYMS = {
    


# ============================================================
# OPTIONAL: Separate creds like your working code
#
# For S3 + Textract (LIST/GET/HEAD + Textract start/get):
#   AWS_ACCESS_KEY_ID_RAG_S3
#   AWS_SECRET_ACCESS_KEY_RAG_S3
#   AWS_SESSION_TOKEN_RAG_S3 (optional)
#
# For Bedrock (invoke_model):
#   AWS_ACCESS_KEY_ID_RAG_BEDROCK
#   AWS_SECRET_ACCESS_KEY_RAG_BEDROCK
#   AWS_SESSION_TOKEN_RAG_BEDROCK (optional)
#
# Leave empty to use default AWS chain (IAM role / ~/.aws).
# ============================================================

def _parse_s3_uri(uri: str) -> Tuple[str, str]:
    u = urlparse(uri)
    bucket = (u.netloc or "").strip()
    key = (u.path or "").lstrip("/")
    if not bucket:
        raise ValueError(f"Invalid S3 URI (missing bucket): {uri}")
    return bucket, key

def _join_key(*parts: str) -> str:
    out = []
    for p in parts:
        p = str(p or "").strip("/")
        if p:
            out.append(p)
    return "/".join(out)

def _make_session_static(region: str, ak: str, sk: str, st: str = "") -> boto3.Session:
    return boto3.Session(
        aws_access_key_id=ak,
        aws_secret_access_key=sk,
        aws_session_token=(st or None),
        region_name=region,
    )

def _get_session_pair(region: str):
    # S3/Textract session
    s3_ak = (os.getenv("AWS_ACCESS_KEY_ID_RAG_S3") or "").strip()
    s3_sk = (os.getenv("AWS_SECRET_ACCESS_KEY_RAG_S3") or "").strip()
    s3_st = (os.getenv("AWS_SESSION_TOKEN_RAG_S3") or "").strip()

    if s3_ak and s3_sk:
        sess_s3 = _make_session_static(region, s3_ak, s3_sk, s3_st)
    else:
        sess_s3 = boto3.Session(region_name=region)

    # Bedrock session
    br_ak = (os.getenv("AWS_ACCESS_KEY_ID_RAG_BEDROCK") or "").strip()
    br_sk = (os.getenv("AWS_SECRET_ACCESS_KEY_RAG_BEDROCK") or "").strip()
    br_st = (os.getenv("AWS_SESSION_TOKEN_RAG_BEDROCK") or "").strip()

    if br_ak and br_sk:
        sess_br = _make_session_static(region, br_ak, br_sk, br_st)
    else:
        sess_br = boto3.Session(region_name=region)

    return sess_s3, sess_br


def norm(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"\s+", " ", s).strip()
    return s

def safe_json_load(s: str) -> Any:
    s = s.strip()
    m = re.search(r"(\{.*\}|\[.*\])", s, re.S)
    if m:
        s = m.group(1)
    return json.loads(s)

def chunk_list(items: List[str], n: int) -> List[List[str]]:
    return [items[i:i+n] for i in range(0, len(items), n)]

def tsv_table(rows: List[List[str]]) -> str:
    return "\n".join("\t".join((c or "").strip() for c in r) for r in rows)

def stable_sort_snippets(snips: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    return sorted(snips, key=lambda x: x["snippet_id"])


# =========================
# Textract (PDF text + tables)
# =========================
def textract_start_and_fetch_all(textract_client, bucket: str, key: str) -> Dict[str, Any]:
    resp = textract_client.start_document_analysis(
        DocumentLocation={"S3Object": {"Bucket": bucket, "Name": key}},
        FeatureTypes=["TABLES"]
    )
    job_id = resp["JobId"]

    while True:
        out = textract_client.get_document_analysis(JobId=job_id)
        status = out["JobStatus"]
        if status in ("SUCCEEDED", "FAILED", "PARTIAL_SUCCESS"):
            break
        time.sleep(2)

    if status != "SUCCEEDED":
        raise RuntimeError(f"Textract job {job_id} status={status}")

    blocks = out.get("Blocks", [])
    next_token = out.get("NextToken")
    while next_token:
        page = textract_client.get_document_analysis(JobId=job_id, NextToken=next_token)
        blocks.extend(page.get("Blocks", []))
        next_token = page.get("NextToken")

    return {"Blocks": blocks}

def textract_blocks_to_tables_and_lines(textract_json: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], List[Tuple[int, str]]]:
    blocks = textract_json.get("Blocks", [])
    by_id = {b["Id"]: b for b in blocks if "Id" in b}

    lines = []
    for b in blocks:
        if b.get("BlockType") == "LINE" and b.get("Text"):
            page = int(b.get("Page", 0) or 0)
            lines.append((page, b["Text"]))

    tables = []
    table_blocks = [b for b in blocks if b.get("BlockType") == "TABLE"]
    for t_i, tb in enumerate(table_blocks, start=1):
        page = int(tb.get("Page", 0) or 0)

        cell_ids = []
        for rel in tb.get("Relationships", []):
            if rel.get("Type") == "CHILD":
                cell_ids.extend(rel.get("Ids", []))

        cells = []
        max_r = 0
        max_c = 0
        for cid in cell_ids:
            cb = by_id.get(cid)
            if not cb or cb.get("BlockType") != "CELL":
                continue
            r = int(cb.get("RowIndex", 1))
            c = int(cb.get("ColumnIndex", 1))
            max_r = max(max_r, r)
            max_c = max(max_c, c)

            words = []
            for rel in cb.get("Relationships", []):
                if rel.get("Type") == "CHILD":
                    for wid in rel.get("Ids", []):
                        wb = by_id.get(wid)
                        if not wb:
                            continue
                        if wb.get("BlockType") == "WORD" and wb.get("Text"):
                            words.append(wb["Text"])
                        elif wb.get("BlockType") == "SELECTION_ELEMENT" and wb.get("SelectionStatus") == "SELECTED":
                            words.append("[X]")
            cells.append((r, c, " ".join(words).strip()))

        matrix = [["" for _ in range(max_c)] for __ in range(max_r)]
        for r, c, txt in cells:
            if 1 <= r <= max_r and 1 <= c <= max_c:
                matrix[r-1][c-1] = txt

        tables.append({"page": page, "table_index": t_i, "matrix": matrix})

    lines.sort(key=lambda x: (x[0], x[1]))
    tables.sort(key=lambda x: (x["page"], x["table_index"]))
    return tables, lines


def build_pdf_snippets_from_textract(company: str, client: str, source_file: str,
                                     textract_json: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    tables, lines = textract_blocks_to_tables_and_lines(textract_json)

    lines_by_page: Dict[int, List[str]] = {}
    for p, txt in lines:
        lines_by_page.setdefault(p, []).append(txt)

    text_snips = []
    for p in sorted(lines_by_page.keys()):
        chunks = chunk_list(lines_by_page[p], PDF_TEXT_LINES_PER_SNIPPET)
        for j, ch in enumerate(chunks, start=1):
            content = "\n".join(ch).strip()
            if content:
                text_snips.append({
                    "snippet_id": f"pdf:{source_file}:p{p}:text{j}",
                    "company": company, "client": client, "source_file": source_file,
                    "type": "text", "loc": {"page": p, "chunk": j}, "content": content
                })

    table_snips = []
    for t in tables:
        matrix = t["matrix"]
        if not matrix:
            continue

        if len(matrix) <= MAX_TABLE_ROWS_PER_CHUNK:
            content = tsv_table(matrix)
            table_snips.append({
                "snippet_id": f"pdf:{source_file}:p{t['page']}:table{t['table_index']}",
                "company": company, "client": client, "source_file": source_file,
                "type": "table",
                "loc": {"page": t["page"], "table": t["table_index"], "row_range": [1, len(matrix)]},
                "content": content
            })
        else:
            header = matrix[:2]
            body = matrix[2:]
            body_chunks = [body[i:i+MAX_TABLE_ROWS_PER_CHUNK] for i in range(0, len(body), MAX_TABLE_ROWS_PER_CHUNK)]
            for k, bch in enumerate(body_chunks, start=1):
                chunk_mat = header + bch
                content = tsv_table(chunk_mat)
                start_row = 1 + 2 + (k-1)*MAX_TABLE_ROWS_PER_CHUNK
                end_row   = 2 + k*MAX_TABLE_ROWS_PER_CHUNK
                table_snips.append({
                    "snippet_id": f"pdf:{source_file}:p{t['page']}:table{t['table_index']}:chunk{k}",
                    "company": company, "client": client, "source_file": source_file,
                    "type": "table",
                    "loc": {"page": t["page"], "table": t["table_index"], "chunk": k, "row_range": [start_row, end_row]},
                    "content": content
                })

    return stable_sort_snippets(text_snips), stable_sort_snippets(table_snips)


def build_docx_snippets(company: str, client: str, path: Path) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    if docx is None:
        raise ImportError("python-docx not installed. Run: pip install python-docx")

    d = docx.Document(str(path))
    source_file = path.name

    paras = [p.text.strip() for p in d.paragraphs if (p.text or "").strip()]
    text_snips = []
    for i, ch in enumerate(chunk_list(paras, DOCX_PARAS_PER_SNIPPET), start=1):
        content = "\n".join(ch).strip()
        if content:
            text_snips.append({
                "snippet_id": f"docx:{source_file}:text{i}",
                "company": company, "client": client, "source_file": source_file,
                "type": "text", "loc": {"para_chunk": i}, "content": content
            })

    table_snips = []
    for t_i, tbl in enumerate(d.tables, start=1):
        rows = []
        for r in tbl.rows:
            rows.append([c.text.strip() for c in r.cells])
        if not rows:
            continue

        if len(rows) <= MAX_TABLE_ROWS_PER_CHUNK:
            content = tsv_table(rows)
            table_snips.append({
                "snippet_id": f"docx:{source_file}:table{t_i}",
                "company": company, "client": client, "source_file": source_file,
                "type": "table", "loc": {"table": t_i, "row_range": [1, len(rows)]},
                "content": content
            })
        else:
            header = rows[:2]
            body = rows[2:]
            body_chunks = [body[i:i+MAX_TABLE_ROWS_PER_CHUNK] for i in range(0, len(body), MAX_TABLE_ROWS_PER_CHUNK)]
            for k, bch in enumerate(body_chunks, start=1):
                content = tsv_table(header + bch)
                start_row = 1 + 2 + (k-1)*MAX_TABLE_ROWS_PER_CHUNK
                end_row   = 2 + k*MAX_TABLE_ROWS_PER_CHUNK
                table_snips.append({
                    "snippet_id": f"docx:{source_file}:table{t_i}:chunk{k}",
                    "company": company, "client": client, "source_file": source_file,
                    "type": "table", "loc": {"table": t_i, "chunk": k, "row_range": [start_row, end_row]},
                    "content": content
                })

    return stable_sort_snippets(text_snips), stable_sort_snippets(table_snips)


def build_xlsx_snippets(company: str, client: str, path: Path) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    if openpyxl is None:
        raise ImportError("openpyxl not installed. Run: pip install openpyxl")

    wb = openpyxl.load_workbook(str(path), data_only=True)
    source_file = path.name

    text_snips = []
    table_snips = []

    for sname in wb.sheetnames:
        sh = wb[sname]
        max_r, max_c = sh.max_row, sh.max_column

        last_r, last_c = 0, 0
        for r in range(1, max_r + 1):
            row_has = False
            for c in range(1, max_c + 1):
                v = sh.cell(row=r, column=c).value
                if v is not None and str(v).strip() != "":
                    row_has = True
                    last_c = max(last_c, c)
            if row_has:
                last_r = r

        if last_r == 0 or last_c == 0:
            continue

        rows = []
        for r in range(1, last_r + 1):
            row = []
            for c in range(1, last_c + 1):
                v = sh.cell(row=r, column=c).value
                row.append("" if v is None else str(v))
            rows.append(row)

        if len(rows) <= MAX_TABLE_ROWS_PER_CHUNK:
            content = tsv_table(rows)
            table_snips.append({
                "snippet_id": f"xlsx:{source_file}:{sname}",
                "company": company, "client": client, "source_file": source_file,
                "type": "table", "loc": {"sheet": sname, "row_range": [1, len(rows)]},
                "content": content
            })
        else:
            header = rows[:2]
            body = rows[2:]
            body_chunks = [body[i:i+MAX_TABLE_ROWS_PER_CHUNK] for i in range(0, len(body), MAX_TABLE_ROWS_PER_CHUNK)]
            for k, bch in enumerate(body_chunks, start=1):
                content = tsv_table(header + bch)
                start_row = 1 + 2 + (k-1)*MAX_TABLE_ROWS_PER_CHUNK
                end_row   = 2 + k*MAX_TABLE_ROWS_PER_CHUNK
                table_snips.append({
                    "snippet_id": f"xlsx:{source_file}:{sname}:chunk{k}",
                    "company": company, "client": client, "source_file": source_file,
                    "type": "table", "loc": {"sheet": sname, "chunk": k, "row_range": [start_row, end_row]},
                    "content": content
                })

    return stable_sort_snippets(text_snips), stable_sort_snippets(table_snips)


# =========================
# Retrieval (deterministic keyword scoring)
# =========================
def score_snippet(field: str, queries: List[str], snippet: Dict[str, Any]) -> float:
    text = norm(snippet["content"])
    if not text:
        return 0.0

    score = 0.0
    for q in queries:
        qn = norm(q)
        if not qn:
            continue
        if qn in text:
            score += 8.0
        toks = [t for t in re.split(r"[^a-z0-9]+", qn) if len(t) >= 3]
        if toks:
            score += sum(1.0 for t in toks if t in text)

    # mild boosts
    if re.search(r"\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b", snippet["content"]):
        score += 0.5
    if re.search(r"[$€£]\s?\d", snippet["content"]) or re.search(r"\b\d{1,3}(,\d{3})+(\.\d+)?\b", snippet["content"]):
        score += 0.5

    return score

def select_top_snippets_for_fields(fields: List[str], snippets: List[Dict[str, Any]], k_per_field: int,
                                  max_total: int) -> List[Dict[str, Any]]:
    snippets = stable_sort_snippets(snippets)
    chosen_ids = []
    chosen_set = set()

    for f in fields:
        queries = [f] + FIELD_SYNONYMS.get(f, [])
        scored = []
        for sn in snippets:
            sc = score_snippet(f, queries, sn)
            if sc > 0:
                scored.append((sc, sn["snippet_id"]))
        scored.sort(key=lambda x: (-x[0], x[1]))  # deterministic
        for _, sid in scored[:k_per_field]:
            if sid not in chosen_set:
                chosen_set.add(sid)
                chosen_ids.append(sid)
        if len(chosen_ids) >= max_total:
            break

    sid2sn = {sn["snippet_id"]: sn for sn in snippets}
    return [sid2sn[sid] for sid in chosen_ids if sid in sid2sn][:max_total]


# =========================
# Bedrock Claude call
# =========================
def bedrock_claude_text(br_client, prompt: str, max_tokens: int = 1700) -> str:
    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "temperature": TEMPERATURE,
        "max_tokens": max_tokens,
        "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
    }
    resp = br_client.invoke_model(
        modelId=BEDROCK_MODEL_ID,
        body=json.dumps(body).encode("utf-8"),
        contentType="application/json",
        accept="application/json",
    )
    raw = resp["body"].read().decode("utf-8")
    data = json.loads(raw)
    if isinstance(data, dict) and "content" in data and data["content"]:
        return "".join([c.get("text","") for c in data["content"] if c.get("type") == "text"]).strip()
    return raw.strip()


def build_prompt(fields: List[str], snippets: List[Dict[str, Any]], phase_label: str) -> str:
    snippets = stable_sort_snippets(snippets)
    evidence_blocks = []
    for sn in snippets:
        evidence_blocks.append(
            f"SNIPPET_ID: {sn['snippet_id']}\n"
            f"SOURCE: {sn['source_file']}\n"
            f"TYPE: {sn['type']}\n"
            f"LOC: {json.dumps(sn['loc'], ensure_ascii=False)}\n"
            f"CONTENT:\n{sn['content']}\n"
        )
    return f"""You are extracting structured fields for one client from provided evidence snippets.

PHASE: {phase_label}

RULES (STRICT):
- Use ONLY the provided snippets as evidence.
- If a field is not explicitly supported by the snippets, output null for that field.
- Output MUST be valid JSON ONLY. No extra text.
- For each field, return an object: {{"value": <string|null>, "snippet_id": <string|null>}}
- snippet_id must be one of the provided SNIPPET_ID values when value is not null.

FIELDS:
{json.dumps(fields, ensure_ascii=False, indent=2)}

EVIDENCE SNIPPETS:
{'\n---\n'.join(evidence_blocks)}
"""

def validate_and_prune(result: Dict[str, Any], snippet_map: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
    cleaned = {}
    for field in FIELDS:
        obj = result.get(field)
        if not isinstance(obj, dict):
            cleaned[field] = {"value": None, "snippet_id": None}
            continue

        value = obj.get("value", None)
        sid   = obj.get("snippet_id", None)
        if value is None or sid is None or sid not in snippet_map:
            cleaned[field] = {"value": None, "snippet_id": None}
            continue

        v = str(value).strip()
        if not v:
            cleaned[field] = {"value": None, "snippet_id": None}
            continue

        content = snippet_map[sid]["content"]
        if norm(v) in norm(content):
            cleaned[field] = {"value": v, "snippet_id": sid}
        else:
            cleaned[field] = {"value": None, "snippet_id": None}

    return cleaned


# =========================
# Main runner (ONE company/client)
# =========================
def run_single_company_client(company: str, client: str) -> Dict[str, Any]:
    base_dir = DATA_ROOT / company / client
    if not base_dir.exists():
        raise FileNotFoundError(f"Local folder not found: {base_dir.resolve()}")

    files = sorted([p for p in base_dir.rglob("*") if p.is_file() and p.suffix.lower() in (".pdf", ".docx", ".xlsx")])
    if not files:
        raise FileNotFoundError(f"No pdf/docx/xlsx found under {base_dir.resolve()}")

    bucket, base_prefix = _parse_s3_uri(S3_PDF_DIR.rstrip("/"))
    base_prefix = base_prefix.rstrip("/")

    sess_s3, sess_br = _get_session_pair(AWS_REGION)
    s3 = sess_s3.client("s3", region_name=AWS_REGION)
    textract = sess_s3.client("textract", region_name=AWS_REGION)
    brt = sess_br.client("bedrock-runtime", region_name=AWS_REGION)

    all_text_snips: List[Dict[str, Any]] = []
    all_table_snips: List[Dict[str, Any]] = []

    for p in files:
        ext = p.suffix.lower()

        if ext == ".pdf":
            source_file = p.name
            s3_key = _join_key(base_prefix, company, client, source_file)

            # --- CRITICAL sanity check (explains "unable to get metadata") ---
            try:
                s3.head_object(Bucket=bucket, Key=s3_key)
            except Exception as e:
                print("\n[S3 HEAD_OBJECT FAILED]")
                print("Bucket:", bucket)
                print("Key:", s3_key)
                print("Error:", repr(e))
                print("\nFix checklist:")
                print("1) Verify the key exists: aws s3 ls s3://%s/%s --recursive" % (bucket, _join_key(base_prefix, company, client)))
                print("2) Ensure your S3 creds (AWS_*_RAG_S3) have s3:GetObject on that prefix.")
                print("3) If SSE-KMS: ensure kms:Decrypt for the object's KMS key.")
                raise

            # Textract call (tables + lines)
            tex_json = textract_start_and_fetch_all(textract, bucket, s3_key)
            t_sn, tab_sn = build_pdf_snippets_from_textract(company, client, source_file, tex_json)
            all_text_snips.extend(t_sn)
            all_table_snips.extend(tab_sn)

        elif ext == ".docx":
            t_sn, tab_sn = build_docx_snippets(company, client, p)
            all_text_snips.extend(t_sn)
            all_table_snips.extend(tab_sn)

        elif ext == ".xlsx":
            t_sn, tab_sn = build_xlsx_snippets(company, client, p)
            all_text_snips.extend(t_sn)
            all_table_snips.extend(tab_sn)

    all_text_snips  = stable_sort_snippets(all_text_snips)
    all_table_snips = stable_sort_snippets(all_table_snips)

    print(f"Collected snippets for {company}/{client}: text={len(all_text_snips)}, tables={len(all_table_snips)}")

    # PASS 1: TEXT ONLY
    text_candidates = select_top_snippets_for_fields(FIELDS, all_text_snips, K_TEXT_PER_FIELD, MAX_TEXT_SNIPPETS_TOTAL)
    text_map = {sn["snippet_id"]: sn for sn in text_candidates}
    raw1 = bedrock_claude_text(brt, build_prompt(FIELDS, text_candidates, "PASS_1_TEXT_ONLY"))
    res1 = safe_json_load(raw1)
    if not isinstance(res1, dict):
        raise ValueError("Model did not return JSON object in PASS 1")
    pruned1 = validate_and_prune(res1, text_map)

    missing = [f for f in FIELDS if not pruned1.get(f, {}).get("value")]
    print(f"PASS 1: Found {len(FIELDS)-len(missing)}/{len(FIELDS)} from TEXT. Missing={len(missing)}")

    final = dict(pruned1)

    # PASS 2: TABLES ONLY FOR MISSING
    if missing and all_table_snips:
        table_candidates = select_top_snippets_for_fields(missing, all_table_snips, K_TABLE_PER_FIELD, MAX_TABLE_SNIPPETS_TOTAL)
        table_map = {sn["snippet_id"]: sn for sn in table_candidates}
        raw2 = bedrock_claude_text(brt, build_prompt(missing, table_candidates, "PASS_2_TABLE_ONLY_MISSING"))
        res2 = safe_json_load(raw2)
        if not isinstance(res2, dict):
            raise ValueError("Model did not return JSON object in PASS 2")
        pruned2 = validate_and_prune(res2, table_map)

        for f in missing:
            if pruned2.get(f, {}).get("value"):
                final[f] = pruned2[f]

    print("\n=== FINAL ANSWERS ===")
    for f in FIELDS:
        v = final.get(f, {}).get("value") or ""
        sid = final.get(f, {}).get("snippet_id") or ""
        print(f"- {f}: {v}  [{sid}]")

    return final


# =========================
# RUN
# =========================
results = run_single_company_client(COMPANY, CLIENT)
