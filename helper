# Paste this as ONE SINGLE CELL in Jupyter to debug FAISS retrieval + Claude extraction in real time.

import json, os, re
from pathlib import Path

import faiss
import numpy as np
import boto3


# ========= 1) Load config, evidence, faiss =========
cfg = json.loads(Path("config.json").read_text(encoding="utf-8"))

evidence_path = Path(cfg["evidence_jsonl_path"])
faiss_path = cfg["faiss_index_path"]

evidence = []
with open(evidence_path, "r", encoding="utf-8") as f:
    for line in f:
        evidence.append(json.loads(line))

print("evidence rows:", len(evidence))
index = faiss.read_index(faiss_path)
print("faiss ntotal:", index.ntotal)


# ========= 2) Bedrock setup =========
AWS_REGION = cfg.get("aws_region", "us-east-1")
BEDROCK_EMBED_MODEL_ID = os.getenv("BEDROCK_EMBED_MODEL_ID")
BEDROCK_LLM_MODEL_ID = os.getenv("BEDROCK_LLM_MODEL_ID")

print("AWS_REGION:", AWS_REGION)
print("embed model:", BEDROCK_EMBED_MODEL_ID)
print("llm model:", BEDROCK_LLM_MODEL_ID)

if not BEDROCK_EMBED_MODEL_ID or not BEDROCK_LLM_MODEL_ID:
    raise RuntimeError("Set env vars: BEDROCK_EMBED_MODEL_ID and BEDROCK_LLM_MODEL_ID")

session = boto3.Session(region_name=AWS_REGION)
bedrock = session.client("bedrock-runtime")


def embed_titan(text: str) -> np.ndarray:
    resp = bedrock.invoke_model(
        modelId=BEDROCK_EMBED_MODEL_ID,
        body=json.dumps({"inputText": text}),
        accept="application/json",
        contentType="application/json",
    )
    vec = json.loads(resp["body"].read())["embedding"]
    v = np.array(vec, dtype="float32")
    n = np.linalg.norm(v)
    if n > 0:
        v = v / n
    return v.reshape(1, -1)


def call_claude(prompt: str) -> str:
    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "temperature": 0,
        "max_tokens": 1200,
        "messages": [
            {"role": "user", "content": [{"type": "text", "text": prompt}]}
        ],
    }
    resp = bedrock.invoke_model(
        modelId=BEDROCK_LLM_MODEL_ID,
        body=json.dumps(body),
        accept="application/json",
        contentType="application/json",
    )
    raw = json.loads(resp["body"].read())
    if isinstance(raw, dict) and "content" in raw and raw["content"]:
        return raw["content"][0].get("text", "")
    return json.dumps(raw)


def normalize(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def prune_fields(fields_dict, evidence_text):
    ev = normalize(evidence_text)
    cleaned = {}
    for k, v in (fields_dict or {}).items():
        if isinstance(v, str) and v.strip():
            if normalize(v) in ev:
                cleaned[k] = v
    return cleaned


# ========= 3) Pick a client to test =========
pairs = sorted(set((x.get("company"), x.get("client_id")) for x in evidence if x.get("company") and x.get("client_id")))
print("pairs found:", len(pairs))
print("first 10 pairs:", pairs[:10])

TEST_COMPANY, TEST_CLIENT = pairs[0]   # <-- change this if you want
# TEST_COMPANY, TEST_CLIENT = "Koch", "client1"
print("\nTESTING:", TEST_COMPANY, TEST_CLIENT)


# ========= 4) Retrieve evidence for that client =========
FIELDS = [
    "airlines included in the contract",
    "OSI",
    "TKT Designator",
    "TOUR CODE",
    "Contact",
    "Global Account Manager",
    "contract Expiry date",
    "GDS fare loading codes",
    "Air corporate Loyality Program",
    "Perks ID",
    "Point of sale validity which market this deal can be sold from?",
    "fare/route details and ticketing instruction received",
    "status",
    "conteact expire",
]

query = " | ".join(FIELDS)
qvec = embed_titan(query)

TOP_K = int(cfg.get("top_k_retrieve", 250))
scores, ids = index.search(qvec, TOP_K)

candidates = []
for doc_id in ids[0].tolist():
    if doc_id < 0:
        continue
    it = evidence[doc_id]
    if it.get("company") == TEST_COMPANY and it.get("client_id") == TEST_CLIENT:
        candidates.append(it)

print("retrieved candidates for client:", len(candidates))

print("\n--- first 3 candidates (preview) ---")
for i, it in enumerate(candidates[:3], start=1):
    print("\n#", i, "|", it.get("type"), "|", it.get("file"), "|", it.get("loc"))
    print(it.get("content", "")[:600])


# ========= 5) Call Claude with a small evidence pack =========
picked = candidates[:30]  # increase to 60 if needed
evidence_blob = "\n\n".join([x.get("content", "") for x in picked])

prompt = (
    "You are a strict extraction engine.\n\n"
    "Return ONLY a JSON object.\n"
    "Include only fields you find verbatim in evidence.\n"
    "If not found, omit the field.\n\n"
    f"Company: {TEST_COMPANY}\n"
    f"Client: {TEST_CLIENT}\n\n"
    "Fields:\n- " + "\n- ".join(FIELDS) + "\n\n"
    "Output format:\n"
    "{\n"
    '  "Company": "<company>",\n'
    '  "Client": "<client>",\n'
    '  "fields": {\n'
    '    "<Field Name>": "<verbatim value>"\n'
    "  }\n"
    "}\n\n"
    "EVIDENCE:\n"
    + evidence_blob
)

print("\n--- calling Claude ---")
out = call_claude(prompt)
print("\n--- Claude raw output (first 4000 chars) ---\n")
print(out[:4000])


# ========= 6) Parse + prune (whitespace-safe) =========
try:
    parsed = json.loads(out)
except Exception as e:
    parsed = {"fields": {}}
    print("\n[warn] Claude output was not valid JSON:", e)

found = prune_fields(parsed.get("fields", {}), evidence_blob)

print("\n--- PRUNED FIELDS (what should go to Excel) ---")
print(found)

# Helpful diagnosis
if len(candidates) == 0:
    print("\nDIAGNOSIS: FAISS retrieval returned 0 items for this client after filtering.")
    print("Likely fix: do per-field retrieval (union results) instead of one big query, or increase TOP_K.")
elif isinstance(parsed.get("fields", {}), dict) and len(parsed.get("fields", {})) == 0:
    print("\nDIAGNOSIS: Claude returned empty fields. Either evidence doesn't contain these items or prompt/evidence size is insufficient.")
    print("Try increasing picked = candidates[:60] or improve field synonyms.")
elif len(parsed.get("fields", {})) > 0 and len(found) == 0:
    print("\nDIAGNOSIS: Claude returned values but prune removed them. The normalize+prune may still be too strict for your formats.")
    print("Fix: store structured citations (ref + loc) per field, or relax prune to allow minor formatting differences.")
