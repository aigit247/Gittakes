import json, os, re, time
from pathlib import Path

import faiss
import numpy as np
import boto3


# ----------------- load config/evidence/faiss -----------------
cfg = json.loads(Path("config.json").read_text(encoding="utf-8"))
AWS_REGION = cfg.get("aws_region", "us-east-1")

evidence_path = Path(cfg["evidence_jsonl_path"])
faiss_path = cfg["faiss_index_path"]

evidence = []
with open(evidence_path, "r", encoding="utf-8") as f:
    for line in f:
        evidence.append(json.loads(line))

index = faiss.read_index(faiss_path)

print("[dbg] evidence rows:", len(evidence))
print("[dbg] faiss ntotal:", index.ntotal)
print("[dbg] sample keys:", list(evidence[0].keys()) if evidence else None)


# ----------------- bedrock clients -----------------
BEDROCK_EMBED_MODEL_ID = os.getenv("BEDROCK_EMBED_MODEL_ID")
BEDROCK_LLM_MODEL_ID = os.getenv("BEDROCK_LLM_MODEL_ID")

print("[dbg] AWS_REGION:", AWS_REGION)
print("[dbg] EMBED_MODEL:", BEDROCK_EMBED_MODEL_ID)
print("[dbg] LLM_MODEL:", BEDROCK_LLM_MODEL_ID)

if not BEDROCK_EMBED_MODEL_ID or not BEDROCK_LLM_MODEL_ID:
    raise RuntimeError("Set env vars BEDROCK_EMBED_MODEL_ID and BEDROCK_LLM_MODEL_ID")

session = boto3.Session(region_name=AWS_REGION)
bedrock = session.client("bedrock-runtime")


# ----------------- helpers -----------------
def embed_titan(text: str) -> np.ndarray:
    resp = bedrock.invoke_model(
        modelId=BEDROCK_EMBED_MODEL_ID,
        body=json.dumps({"inputText": text}),
        accept="application/json",
        contentType="application/json",
    )
    vec = json.loads(resp["body"].read())["embedding"]
    v = np.array(vec, dtype="float32")
    n = np.linalg.norm(v)
    if n > 0:
        v = v / n
    return v.reshape(1, -1)

def extract_text_any(raw: dict) -> str:
    # common Bedrock Anthropic messages output
    if isinstance(raw, dict):
        if "content" in raw and isinstance(raw["content"], list) and raw["content"]:
            first = raw["content"][0]
            if isinstance(first, dict) and "text" in first:
                return first["text"]
        out = raw.get("output", {})
        msg = out.get("message", {})
        cont = msg.get("content", [])
        if isinstance(cont, list) and cont and isinstance(cont[0], dict) and "text" in cont[0]:
            return cont[0]["text"]
        if "completion" in raw and isinstance(raw["completion"], str):
            return raw["completion"]
    return ""

def call_claude(prompt: str) -> str:
    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "temperature": 0,
        "max_tokens": 1400,
        "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
    }
    resp = bedrock.invoke_model(
        modelId=BEDROCK_LLM_MODEL_ID,
        body=json.dumps(body),
        accept="application/json",
        contentType="application/json",
    )
    raw_text = resp["body"].read().decode("utf-8", errors="replace")
    try:
        raw_json = json.loads(raw_text)
    except Exception:
        print("\n[dbg] Bedrock raw body (first 800 chars):")
        print(raw_text[:800])
        return ""
    return extract_text_any(raw_json)

def normalize(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def prune_fields(fields_dict, evidence_text):
    ev = normalize(evidence_text)
    cleaned = {}
    for k, v in (fields_dict or {}).items():
        if isinstance(v, str) and v.strip():
            if normalize(v) in ev:
                cleaned[k] = v
    return cleaned

def stable_sort_key(it):
    type_rank = 0 if it.get("type") == "TABLE_ROW" else 1
    file_name = it.get("file", "")
    loc = it.get("loc", "")
    table_id = it.get("table_id") or ""
    row = it.get("row_index") if it.get("row_index") is not None else 10**9
    return (type_rank, file_name, loc, table_id, row, it.get("doc_id", 0))

def pick_evidence(items, max_units=80, max_table=50, max_text=30):
    items = list(items)
    items.sort(key=stable_sort_key)
    picked, t_count, x_count = [], 0, 0
    for it in items:
        if it.get("type") == "TABLE_ROW":
            if t_count >= max_table:
                continue
            t_count += 1
        else:
            if x_count >= max_text:
                continue
            x_count += 1
        picked.append(it)
        if len(picked) >= max_units:
            break
    return picked

def retrieve_union_for_fields(company, client, fields, top_each=25, top_global=250):
    """
    Deterministic union retrieval:
    - search each field separately -> top_each
    - union doc_ids
    - plus one global combined query -> top_global
    - filter to company/client
    """
    all_ids = set()

    # per-field
    for f in fields:
        qvec = embed_titan(f)
        _, ids = index.search(qvec, top_each)
        for doc_id in ids[0].tolist():
            if doc_id < 0:
                continue
            it = evidence[doc_id]
            if it.get("company") == company and it.get("client_id") == client:
                all_ids.add(doc_id)

    # global fallback
    qvec = embed_titan(" | ".join(fields))
    _, ids = index.search(qvec, top_global)
    for doc_id in ids[0].tolist():
        if doc_id < 0:
            continue
        it = evidence[doc_id]
        if it.get("company") == company and it.get("client_id") == client:
            all_ids.add(doc_id)

    items = [evidence[i] for i in sorted(all_ids)]
    return items


# ----------------- choose company/client -----------------
pairs = sorted(set((x.get("company"), x.get("client_id")) for x in evidence if x.get("company") and x.get("client_id")))
print("\n[dbg] pairs:", len(pairs))
print("[dbg] first 10:", pairs[:10])

TEST_COMPANY, TEST_CLIENT = pairs[0]  # <-- change if needed
# TEST_COMPANY, TEST_CLIENT = "Koch", "client1"
print("\n[dbg] TESTING:", TEST_COMPANY, TEST_CLIENT)


# ----------------- fields (+ optional synonyms for retrieval only) -----------------
FIELDS = [
    "airlines included in the contract",
    "OSI",
    "TKT Designator",
    "TOUR CODE",
    "Contact",
    "Global Account Manager",
    "contract Expiry date",
    "GDS fare loading codes",
    "Air corporate Loyality Program",
    "Perks ID",
    "Point of sale validity which market this deal can be sold from?",
    "fare/route details and ticketing instruction received",
    "status",
    "conteact expire",
]

# Optional: retrieval synonyms only (output still uses your original field name)
FIELD_SYNONYMS = {
    "contract Expiry date": ["expiration date", "expiry date", "valid until", "validity", "expires"],
    "TOUR CODE": ["tourcode", "tour code", "tour-code"],
    "TKT Designator": ["ticket designator", "tkt designator"],
    "OSI": ["other service information", "OSI"],
    "GDS fare loading codes": ["fare loading code", "GDS code", "loading code"],
    "Air corporate Loyality Program": ["corporate loyalty program", "loyalty program"],
    "Contact": ["contact", "point of contact", "POC"],
    "status": ["status", "current status"],
}

# Expand retrieval queries deterministically
retrieval_queries = []
for f in FIELDS:
    retrieval_queries.append(f)
    for syn in FIELD_SYNONYMS.get(f, []):
        retrieval_queries.append(syn)


# ----------------- A) Retrieve evidence per field (union) -----------------
t0 = time.time()
candidates = retrieve_union_for_fields(
    TEST_COMPANY, TEST_CLIENT, retrieval_queries,
    top_each=25,           # per-field top
    top_global=250         # global fallback top
)
print(f"\n[dbg] union candidates: {len(candidates)} (took {time.time()-t0:.2f}s)")

print("\n[dbg] preview 5 candidates:")
for it in sorted(candidates, key=stable_sort_key)[:5]:
    print("-", it.get("type"), "|", it.get("file"), "|", it.get("loc"))
    print("  ", (it.get("content","")[:220]).replace("\n", " ") + ("..." if len(it.get("content","")) > 220 else ""))


# ----------------- Choose stable evidence pack -----------------
picked = pick_evidence(
    candidates,
    max_units=int(cfg.get("max_evidence_units", 90)),
    max_table=int(cfg.get("max_table_units", 60)),
    max_text=int(cfg.get("max_text_units", 30)),
)
print("\n[dbg] picked evidence units:", len(picked))

# Make evidence blob
evidence_blob = "\n\n".join([x.get("content", "") for x in picked])


# ----------------- B) Prompt Claude once per client -----------------
prompt = (
    "You are a strict extraction engine.\n\n"
    "Rules:\n"
    "1) Use ONLY the provided evidence.\n"
    "2) Return ONLY a valid JSON object. No markdown. No commentary.\n"
    "3) ONLY include fields where the VALUE can be copied verbatim from evidence.\n"
    "4) If not found, OMIT the field.\n"
    "5) Do not infer, normalize, or compute.\n\n"
    "Output format:\n"
    "{\n"
    f'  "Company": "{TEST_COMPANY}",\n'
    f'  "Client": "{TEST_CLIENT}",\n'
    '  "fields": {\n'
    '    "<Field Name>": "<verbatim value>"\n'
    "  }\n"
    "}\n\n"
    "Fields:\n- " + "\n- ".join(FIELDS) + "\n\n"
    "EVIDENCE:\n" + evidence_blob
)

print("\n[dbg] calling Claude...")
out_text = call_claude(prompt)
print("\n[dbg] Claude extracted text (first 2000 chars):\n")
print((out_text or "")[:2000])

if not out_text:
    print("\n[dbg] Claude returned empty text. Most likely model payload mismatch or permission/validation error.")
    print("      In that case, print the Bedrock raw body in call_claude() and check your modelId.")
    raise SystemExit


# ----------------- Parse JSON -----------------
try:
    parsed = json.loads(out_text)
except Exception as e:
    print("\n[dbg] JSON parse error:", e)
    print("\n[dbg] This means Claude did NOT return pure JSON. Fix by making prompt stricter OR add a JSON-repair step.")
    raise

raw_fields = parsed.get("fields", {}) if isinstance(parsed, dict) else {}
print("\n[dbg] fields returned by Claude:", list(raw_fields.keys())[:50])


# ----------------- C) Prune using normalized substring -----------------
pruned = prune_fields(raw_fields, evidence_blob)
print("\n[dbg] fields after prune:", list(pruned.keys())[:50])


# ----------------- D) Show final row (blank if missing) -----------------
final_row = {"Company": TEST_COMPANY, "Client": TEST_CLIENT}
for f in FIELDS:
    final_row[f] = pruned.get(f, "")

print("\n========== FINAL ROW OUTPUT ==========")
for k, v in final_row.items():
    if k in ("Company", "Client"):
        print(f"{k}: {v}")
print("\nFields:")
for f in FIELDS:
    if final_row[f]:
        print(f"- {f}: {final_row[f]}")
    else:
        print(f"- {f}: (blank)")


  # ragservice.py
# Deterministic RAG extraction:
# A) Retrieve evidence per field (union)
# B) One Claude call per (Company, Client)
# C) Prune using normalized substring
# D) If field not found -> blank in Excel

import json
import os
import re
from pathlib import Path

import boto3
import faiss
import numpy as np
import pandas as pd


def load_config(path: str = "config.json") -> dict:
    return json.loads(Path(path).read_text(encoding="utf-8"))


def load_evidence(path: Path):
    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            items.append(json.loads(line))

    # stable ordering helps reproducibility
    items.sort(
        key=lambda x: (
            x.get("company", ""),
            x.get("client_id", ""),
            x.get("file", ""),
            x.get("doc_id", 0),
        )
    )
    return items


def allowed_company(company: str, include, exclude) -> bool:
    include = include or []
    exclude = exclude or []
    if include and company not in include:
        return False
    if exclude and company in exclude:
        return False
    return True


def normalize(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s


def prune_fields(fields_dict, evidence_text):
    # Keep only values that appear in evidence (whitespace-normalized)
    ev = normalize(evidence_text)
    cleaned = {}
    for k, v in (fields_dict or {}).items():
        if isinstance(v, str) and v.strip():
            if normalize(v) in ev:
                cleaned[k] = v
    return cleaned


def embed_titan(bedrock, model_id: str, text: str) -> np.ndarray:
    resp = bedrock.invoke_model(
        modelId=model_id,
        body=json.dumps({"inputText": text}),
        accept="application/json",
        contentType="application/json",
    )
    vec = json.loads(resp["body"].read())["embedding"]
    v = np.array(vec, dtype="float32")
    n = np.linalg.norm(v)
    if n > 0:
        v = v / n
    return v.reshape(1, -1)


def extract_text_any(raw: dict) -> str:
    # Common Bedrock Anthropic Messages output shapes
    if isinstance(raw, dict):
        if "content" in raw and isinstance(raw["content"], list) and raw["content"]:
            first = raw["content"][0]
            if isinstance(first, dict) and "text" in first:
                return first["text"]

        out = raw.get("output", {})
        msg = out.get("message", {})
        cont = msg.get("content", [])
        if isinstance(cont, list) and cont and isinstance(cont[0], dict) and "text" in cont[0]:
            return cont[0]["text"]

        if "completion" in raw and isinstance(raw["completion"], str):
            return raw["completion"]

    return ""


def call_claude(bedrock, model_id: str, prompt: str) -> str:
    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "temperature": 0,
        "max_tokens": 1600,
        "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
    }

    resp = bedrock.invoke_model(
        modelId=model_id,
        body=json.dumps(body),
        accept="application/json",
        contentType="application/json",
    )

    raw_text = resp["body"].read().decode("utf-8", errors="replace")
    raw_json = json.loads(raw_text)
    return extract_text_any(raw_json)


def stable_sort_key(it):
    # deterministic ordering for evidence selection
    type_rank = 0 if it.get("type") == "TABLE_ROW" else 1
    file_name = it.get("file", "")
    loc = it.get("loc", "")
    table_id = it.get("table_id") or ""
    row = it.get("row_index") if it.get("row_index") is not None else 10**9
    return (type_rank, file_name, loc, table_id, row, it.get("doc_id", 0))


def pick_evidence(items, max_units=90, max_table=60, max_text=30):
    items = list(items)
    items.sort(key=stable_sort_key)

    picked, t_count, x_count = [], 0, 0
    for it in items:
        if it.get("type") == "TABLE_ROW":
            if t_count >= max_table:
                continue
            t_count += 1
        else:
            if x_count >= max_text:
                continue
            x_count += 1

        picked.append(it)
        if len(picked) >= max_units:
            break

    return picked


def expand_retrieval_queries(fields, field_synonyms):
    queries = []
    for f in fields:
        queries.append(f)
        for syn in field_synonyms.get(f, []):
            queries.append(syn)
    return queries


def retrieve_union_for_fields(
    evidence_items,
    faiss_index,
    bedrock,
    embed_model_id: str,
    company: str,
    client_id: str,
    queries,
    top_each: int = 25,
    top_global: int = 250,
):
    """
    Deterministic union retrieval:
      - per query: top_each -> union
      - plus one global query of all fields: top_global -> union
      - filter to company/client
    """
    all_ids = set()

    # per-field / per-synonym queries
    for q in queries:
        qvec = embed_titan(bedrock, embed_model_id, q)
        _, ids = faiss_index.search(qvec, top_each)
        for doc_id in ids[0].tolist():
            if doc_id < 0:
                continue
            it = evidence_items[doc_id]
            if it.get("company") == company and it.get("client_id") == client_id:
                all_ids.add(doc_id)

    # global fallback query
    global_q = " | ".join(queries[: min(len(queries), 60)])  # keep bounded
    qvec = embed_titan(bedrock, embed_model_id, global_q)
    _, ids = faiss_index.search(qvec, top_global)
    for doc_id in ids[0].tolist():
        if doc_id < 0:
            continue
        it = evidence_items[doc_id]
        if it.get("company") == company and it.get("client_id") == client_id:
            all_ids.add(doc_id)

    return [evidence_items[i] for i in sorted(all_ids)]


def build_prompt(company: str, client_id: str, fields, evidence_units):
    blocks = []
    for i, it in enumerate(evidence_units, start=1):
        ref = f"EVID_{i}"
        meta = {
            "ref": ref,
            "company": it.get("company"),
            "client": it.get("client_id"),
            "file": it.get("file"),
            "filetype": it.get("filetype"),
            "type": it.get("type"),
            "loc": it.get("loc"),
            "table_id": it.get("table_id"),
            "row_index": it.get("row_index"),
        }
        blocks.append(f"[{ref}]\nMETA: {json.dumps(meta)}\nCONTENT:\n{it.get('content','')}")

    evidence_text = "\n\n".join(blocks)

    prompt = (
        "You are a strict extraction engine.\n\n"
        "Rules:\n"
        "1) Use ONLY the provided evidence.\n"
        "2) Return ONLY a valid JSON object. No markdown. No commentary.\n"
        "3) ONLY include fields where the VALUE can be copied verbatim from evidence.\n"
        "4) If not found, OMIT the field.\n"
        "5) Do not infer, normalize, or compute.\n\n"
        "Output format:\n"
        "{\n"
        f'  "Company": "{company}",\n'
        f'  "Client": "{client_id}",\n'
        '  "fields": {\n'
        '    "<Field Name>": "<verbatim value>"\n'
        "  }\n"
        "}\n\n"
        "Fields:\n- " + "\n- ".join(fields) + "\n\n"
        "EVIDENCE:\n" + evidence_text
    )

    return prompt, evidence_text


def main():
    cfg = load_config()

    region = cfg.get("aws_region", "us-east-1")
    evidence_path = Path(cfg["evidence_jsonl_path"])
    faiss_path = cfg["faiss_index_path"]
    excel_path = Path(cfg["excel_path"])

    include = cfg.get("include_companies", []) or []
    exclude = cfg.get("exclude_companies", []) or []

    # retrieval knobs
    top_each = int(cfg.get("top_each_per_field", 25))       # you can add to config.json if you want
    top_global = int(cfg.get("top_k_retrieve", 250))        # existing config key

    # evidence pack knobs (stable/deterministic)
    max_units = int(cfg.get("max_evidence_units", 90))
    max_table = int(cfg.get("max_table_units", 60))
    max_text = int(cfg.get("max_text_units", 30))

    embed_model_id = os.getenv("BEDROCK_EMBED_MODEL_ID")
    llm_model_id = os.getenv("BEDROCK_LLM_MODEL_ID")
    if not embed_model_id or not llm_model_id:
        raise RuntimeError("Set env vars: BEDROCK_EMBED_MODEL_ID and BEDROCK_LLM_MODEL_ID")

    session = boto3.Session(region_name=region)
    bedrock = session.client("bedrock-runtime")

    evidence_items = load_evidence(evidence_path)
    faiss_index = faiss.read_index(faiss_path)

    # Your extraction fields
    FIELDS = [
        "airlines included in the contract",
        "OSI",
        "TKT Designator",
        "TOUR CODE",
        "Contact",
        "Global Account Manager",
        "contract Expiry date",
        "GDS fare loading codes",
        "Air corporate Loyality Program",
        "Perks ID",
        "Point of sale validity which market this deal can be sold from?",
        "fare/route details and ticketing instruction received",
        "status",
        "conteact expire",
    ]

    # Retrieval synonyms only (output still uses your exact field names)
    FIELD_SYNONYMS = {
        "contract Expiry date": ["expiration date", "expiry date", "valid until", "validity", "expires"],
        "TOUR CODE": ["tourcode", "tour code", "tour-code"],
        "TKT Designator": ["ticket designator", "tkt designator"],
        "OSI": ["other service information", "OSI"],
        "GDS fare loading codes": ["fare loading code", "GDS code", "loading code"],
        "Air corporate Loyality Program": ["corporate loyalty program", "loyalty program"],
        "Contact": ["point of contact", "POC", "contact"],
        "status": ["current status", "status"],
        "fare/route details and ticketing instruction received": ["ticketing instruction", "ticketing instructions", "route details", "fare details"],
    }

    retrieval_queries = expand_retrieval_queries(FIELDS, FIELD_SYNONYMS)

    # Build list of (company, client) pairs from evidence
    pairs = set()
    for it in evidence_items:
        c = it.get("company")
        cl = it.get("client_id")
        if c and cl and allowed_company(c, include, exclude):
            pairs.add((c, cl))
    pairs = sorted(pairs, key=lambda x: (x[0], x[1]))

    if not pairs:
        print("[ragservice] No (company, client) pairs found in evidence.jsonl.")
        return

    rows = []

    for company, client_id in pairs:
        print(f"[ragservice] processing {company}/{client_id}")

        # A) retrieve union evidence (per-field + global fallback)
        candidates = retrieve_union_for_fields(
            evidence_items=evidence_items,
            faiss_index=faiss_index,
            bedrock=bedrock,
            embed_model_id=embed_model_id,
            company=company,
            client_id=client_id,
            queries=retrieval_queries,
            top_each=top_each,
            top_global=top_global,
        )

        picked = pick_evidence(candidates, max_units=max_units, max_table=max_table, max_text=max_text)

        # D) if nothing, blank row
        if not picked:
            row = {"Company": company, "Client": client_id}
            for f in FIELDS:
                row[f] = ""
            rows.append(row)
            continue

        # B) one Claude call per client
        prompt, evidence_text = build_prompt(company, client_id, FIELDS, picked)

        try:
            out_text = call_claude(bedrock, llm_model_id, prompt)
        except Exception as e:
            print(f"  [warn] Claude invoke failed for {company}/{client_id}: {e}")
            row = {"Company": company, "Client": client_id}
            for f in FIELDS:
                row[f] = ""
            rows.append(row)
            continue

        try:
            parsed = json.loads(out_text)
        except Exception:
            # If model didn't return JSON, keep blanks (deterministic)
            row = {"Company": company, "Client": client_id}
            for f in FIELDS:
                row[f] = ""
            rows.append(row)
            continue

        raw_fields = parsed.get("fields", {}) if isinstance(parsed, dict) else {}
        # C) prune with normalized substring
        pruned = prune_fields(raw_fields, evidence_text)

        row = {"Company": company, "Client": client_id}
        for f in FIELDS:
            row[f] = pruned.get(f, "")
        rows.append(row)

    excel_path.parent.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(rows).to_excel(excel_path, index=False)
    print(f"[ragservice] saved -> {excel_path}")


if __name__ == "__main__":
    main()




  ____
