{
  "aws_region": "us-east-1",

  "data_root": "data",
  "textract_json_root": "textract_json",

  "evidence_jsonl_path": "index/evidence.jsonl",
  "faiss_index_path": "index/faiss.index",
  "excel_path": "out/results.xlsx",

  "top_k_retrieve": 250,
  "max_evidence_units": 90,
  "max_table_units": 60,
  "max_text_units": 30,

  "text_block_max_lines": 6,
  "docx_block_max_paras": 6,

  "table_header_match_threshold": 0.85,
  "table_min_consistency": 0.7,

  "include_companies": [],
  "exclude_companies": []
}


import json
from pathlib import Path
from collections import defaultdict, Counter
from difflib import SequenceMatcher

from docx import Document
from openpyxl import load_workbook

def load_config():
    return json.loads(Path("config.json").read_text(encoding="utf-8"))

def clean(s: str) -> str:
    return " ".join((s or "").split()).strip()

def sim(a: str, b: str) -> float:
    return SequenceMatcher(None, a, b).ratio()

def allowed_company(company: str, include, exclude) -> bool:
    include = include or []
    exclude = exclude or []
    if include and company not in include:
        return False
    if exclude and company in exclude:
        return False
    return True

def table_is_consistent(matrix, min_rows=2, min_consistency=0.7):
    if not matrix or len(matrix) < min_rows:
        return False
    nonempty = [r for r in matrix if any(clean(x) for x in r)]
    if len(nonempty) < min_rows:
        return False
    counts = [len(r) for r in nonempty]
    mode = Counter(counts).most_common(1)[0][0]
    ratio = sum(1 for c in counts if c == mode) / len(counts)
    return ratio >= min_consistency

def matrix_to_text(matrix):
    return "\n".join(" | ".join(clean(c) for c in row) for row in matrix)

def canonical_table_row(file_name, filetype, loc, table_id, row_index, headers, values):
    return (
        f"TYPE=TABLE_ROW\n"
        f"FILE={file_name}\n"
        f"FILETYPE={filetype}\n"
        f"LOC={loc}\n"
        f"TABLE_ID={table_id}\n"
        f"ROW={row_index}\n"
        f"HEADERS: {' | '.join(headers)}\n"
        f"VALUES: {' | '.join(values)}"
    )

def canonical_text_block(file_name, filetype, loc, text):
    return (
        f"TYPE=TEXT_BLOCK\n"
        f"FILE={file_name}\n"
        f"FILETYPE={filetype}\n"
        f"LOC={loc}\n"
        f"CONTENT:\n{text}"
    )

# -------- PDF (Textract JSON) ----------
def extract_pdf_lines(textract):
    lines = []
    for b in textract.get("Blocks", []):
        if b.get("BlockType") == "LINE" and b.get("Text"):
            lines.append({"page": int(b.get("Page", 1)), "text": clean(b["Text"])})
    lines.sort(key=lambda x: (x["page"], x["text"]))
    return lines

def extract_pdf_tables(textract):
    blocks = textract.get("Blocks", [])
    by_id = {b["Id"]: b for b in blocks if "Id" in b}
    tables = []

    for b in blocks:
        if b.get("BlockType") != "TABLE":
            continue
        page = int(b.get("Page", 1))

        child_ids = []
        for rel in b.get("Relationships", []):
            if rel.get("Type") == "CHILD":
                child_ids.extend(rel.get("Ids", []))

        grid = defaultdict(dict)
        max_r, max_c = 0, 0

        for cid in child_ids:
            cell = by_id.get(cid)
            if not cell or cell.get("BlockType") != "CELL":
                continue

            r = int(cell.get("RowIndex", 0))
            c = int(cell.get("ColumnIndex", 0))
            if r <= 0 or c <= 0:
                continue

            words = []
            for rel in cell.get("Relationships", []):
                if rel.get("Type") == "CHILD":
                    for wid in rel.get("Ids", []):
                        w = by_id.get(wid)
                        if w and w.get("BlockType") == "WORD" and w.get("Text"):
                            words.append(w["Text"])

            grid[r][c] = clean(" ".join(words))
            max_r = max(max_r, r)
            max_c = max(max_c, c)

        matrix = []
        for r in range(1, max_r + 1):
            row = [grid[r].get(c, "") for c in range(1, max_c + 1)]
            if any(clean(x) for x in row):
                matrix.append(row)

        if matrix:
            tables.append({"page": page, "matrix": matrix})

    tables.sort(key=lambda t: t["page"])
    return tables

def merge_tables_across_pages(page_tables, header_threshold=0.85):
    logical = []
    current = None

    def hdr_str(row):
        return "|".join(clean(x) for x in row)

    for t in page_tables:
        m = t["matrix"]
        page = t["page"]
        if not m:
            continue

        header = [clean(x) for x in m[0]]
        nonempty = sum(1 for x in header if x)

        if current is None:
            current = {"page_start": page, "page_end": page, "header": header, "rows": m[1:]}
            logical.append(current)
            continue

        match = sim(hdr_str(header), hdr_str(current["header"]))
        looks_missing = nonempty <= max(1, len(header) // 4)

        if match >= header_threshold or looks_missing:
            data_rows = m if looks_missing else m[1:]
            current["rows"].extend(data_rows)
            current["page_end"] = page
        else:
            current = {"page_start": page, "page_end": page, "header": header, "rows": m[1:]}
            logical.append(current)

    return logical

def make_pdf_text_blocks(lines, max_lines=6):
    by_page = defaultdict(list)
    for ln in lines:
        by_page[ln["page"]].append(ln["text"])

    blocks = []
    for page in sorted(by_page.keys()):
        buf = []
        for t in by_page[page]:
            buf.append(t)
            if len(buf) == max_lines:
                blocks.append({"page": page, "text": " ".join(buf)})
                buf = []
        if buf:
            blocks.append({"page": page, "text": " ".join(buf)})

    return blocks

# -------- DOCX ----------
def extract_docx_text_blocks(docx_path: Path, max_paras=6):
    doc = Document(str(docx_path))
    paras = [clean(p.text) for p in doc.paragraphs if clean(p.text)]
    blocks, buf = [], []
    for p in paras:
        buf.append(p)
        if len(buf) == max_paras:
            blocks.append(" ".join(buf))
            buf = []
    if buf:
        blocks.append(" ".join(buf))
    return blocks

def extract_docx_tables(docx_path: Path):
    doc = Document(str(docx_path))
    out = []
    for t_i, table in enumerate(doc.tables, start=1):
        rows = [[clean(cell.text) for cell in row.cells] for row in table.rows]
        rows = [r for r in rows if any(clean(x) for x in r)]
        if len(rows) >= 2:
            out.append({"table_id": f"DOCX_T{t_i}", "header": rows[0], "rows": rows[1:]})
    return out

# -------- XLSX ----------
def extract_xlsx_sheets(xlsx_path: Path):
    wb = load_workbook(filename=str(xlsx_path), data_only=True)
    out = []
    for sh in wb.worksheets:
        rows = []
        for r in sh.iter_rows(values_only=True):
            row = [clean("" if v is None else str(v)) for v in r]
            if any(clean(x) for x in row):
                rows.append(row)
        if len(rows) >= 2:
            out.append({"sheet": sh.title, "header": rows[0], "rows": rows[1:]})
    return out

def main():
    cfg = load_config()
    data_root = Path(cfg["data_root"])
    textract_root = Path(cfg["textract_json_root"])
    evidence_path = Path(cfg["evidence_jsonl_path"])
    evidence_path.parent.mkdir(parents=True, exist_ok=True)

    include = cfg.get("include_companies", [])
    exclude = cfg.get("exclude_companies", [])
    header_threshold = float(cfg["table_header_match_threshold"])
    min_consistency = float(cfg["table_min_consistency"])
    pdf_max_lines = int(cfg["text_block_max_lines"])
    docx_max_paras = int(cfg["docx_block_max_paras"])

    doc_id = 0
    total = 0

    with open(evidence_path, "w", encoding="utf-8") as out:
        for company_dir in sorted([p for p in data_root.iterdir() if p.is_dir()], key=lambda p: p.name):
            company = company_dir.name
            if not allowed_company(company, include, exclude):
                continue

            for client_dir in sorted([p for p in company_dir.iterdir() if p.is_dir()], key=lambda p: p.name):
                client_id = client_dir.name

                for file_path in sorted(client_dir.iterdir(), key=lambda p: p.name):
                    if not file_path.is_file():
                        continue

                    suffix = file_path.suffix.lower()
                    file_name = file_path.name

                    # PDF uses textract_json/<company>/<client>/<stem>.json
                    if suffix == ".pdf":
                        json_path = textract_root / company / client_id / f"{file_path.stem}.json"
                        if not json_path.exists():
                            continue

                        textract = json.loads(json_path.read_text(encoding="utf-8"))

                        # tables
                        page_tables = extract_pdf_tables(textract)
                        logical = merge_tables_across_pages(page_tables, header_threshold=header_threshold)

                        for t_i, lt in enumerate(logical, start=1):
                            table_id = f"{file_path.stem}_LT_{t_i}"
                            loc = f"PAGE_RANGE={lt['page_start']}-{lt['page_end']}"
                            full_matrix = [lt["header"]] + lt["rows"]

                            if not table_is_consistent(full_matrix, 2, min_consistency):
                                content = canonical_text_block(
                                    file_name, "pdf", f"PAGE={lt['page_start']}",
                                    "(TABLE_AS_TEXT)\n" + matrix_to_text(full_matrix)
                                )
                                rec = {
                                    "doc_id": doc_id,
                                    "company": company,
                                    "client_id": client_id,
                                    "file": file_name,
                                    "filetype": "pdf",
                                    "type": "TEXT_BLOCK",
                                    "loc": f"PAGE={lt['page_start']}",
                                    "table_id": None,
                                    "row_index": None,
                                    "content": content
                                }
                                out.write(json.dumps(rec, ensure_ascii=False) + "\n")
                                doc_id += 1
                                total += 1
                            else:
                                headers = [clean(x) for x in lt["header"]]
                                for r_i, row in enumerate(lt["rows"], start=1):
                                    values = [clean(x) for x in row]
                                    content = canonical_table_row(file_name, "pdf", loc, table_id, r_i, headers, values)
                                    rec = {
                                        "doc_id": doc_id,
                                        "company": company,
                                        "client_id": client_id,
                                        "file": file_name,
                                        "filetype": "pdf",
                                        "type": "TABLE_ROW",
                                        "loc": loc,
                                        "table_id": table_id,
                                        "row_index": r_i,
                                        "content": content
                                    }
                                    out.write(json.dumps(rec, ensure_ascii=False) + "\n")
                                    doc_id += 1
                                    total += 1

                        # text
                        lines = extract_pdf_lines(textract)
                        blocks = make_pdf_text_blocks(lines, max_lines=pdf_max_lines)
                        for blk in blocks:
                            content = canonical_text_block(file_name, "pdf", f"PAGE={blk['page']}", blk["text"])
                            rec = {
                                "doc_id": doc_id,
                                "company": company,
                                "client_id": client_id,
                                "file": file_name,
                                "filetype": "pdf",
                                "type": "TEXT_BLOCK",
                                "loc": f"PAGE={blk['page']}",
                                "table_id": None,
                                "row_index": None,
                                "content": content
                            }
                            out.write(json.dumps(rec, ensure_ascii=False) + "\n")
                            doc_id += 1
                            total += 1

                    # DOCX
                    elif suffix == ".docx":
                        blocks = extract_docx_text_blocks(file_path, max_paras=docx_max_paras)
                        for i, txt in enumerate(blocks, start=1):
                            loc = f"DOCX_BLOCK={i}"
                            content = canonical_text_block(file_name, "docx", loc, txt)
                            rec = {
                                "doc_id": doc_id,
                                "company": company,
                                "client_id": client_id,
                                "file": file_name,
                                "filetype": "docx",
                                "type": "TEXT_BLOCK",
                                "loc": loc,
                                "table_id": None,
                                "row_index": None,
                                "content": content
                            }
                            out.write(json.dumps(rec, ensure_ascii=False) + "\n")
                            doc_id += 1
                            total += 1

                        tables = extract_docx_tables(file_path)
                        for t in tables:
                            headers = [clean(x) for x in t["header"]]
                            loc = f"DOCX_TABLE={t['table_id']}"
                            full_matrix = [headers] + t["rows"]

                            if not table_is_consistent(full_matrix, 2, min_consistency):
                                content = canonical_text_block(file_name, "docx", loc, "(TABLE_AS_TEXT)\n" + matrix_to_text(full_matrix))
                                rec = {
                                    "doc_id": doc_id,
                                    "company": company,
                                    "client_id": client_id,
                                    "file": file_name,
                                    "filetype": "docx",
                                    "type": "TEXT_BLOCK",
                                    "loc": loc,
                                    "table_id": None,
                                    "row_index": None,
                                    "content": content
                                }
                                out.write(json.dumps(rec, ensure_ascii=False) + "\n")
                                doc_id += 1
                                total += 1
                            else:
                                for r_i, row in enumerate(t["rows"], start=1):
                                    values = [clean(x) for x in row]
                                    content = canonical_table_row(file_name, "docx", loc, t["table_id"], r_i, headers, values)
                                    rec = {
                                        "doc_id": doc_id,
                                        "company": company,
                                        "client_id": client_id,
                                        "file": file_name,
                                        "filetype": "docx",
                                        "type": "TABLE_ROW",
                                        "loc": loc,
                                        "table_id": t["table_id"],
                                        "row_index": r_i,
                                        "content": content
                                    }
                                    out.write(json.dumps(rec, ensure_ascii=False) + "\n")
                                    doc_id += 1
                                    total += 1

                    # XLSX
                    elif suffix == ".xlsx":
                        sheets = extract_xlsx_sheets(file_path)
                        for s in sheets:
                            sheet = s["sheet"]
                            headers = [clean(x) for x in s["header"]]
                            loc = f"SHEET={sheet}"
                            full_matrix = [headers] + s["rows"]

                            if not table_is_consistent(full_matrix, 2, min_consistency):
                                content = canonical_text_block(file_name, "xlsx", loc, "(SHEET_AS_TEXT)\n" + matrix_to_text(full_matrix))
                                rec = {
                                    "doc_id": doc_id,
                                    "company": company,
                                    "client_id": client_id,
                                    "file": file_name,
                                    "filetype": "xlsx",
                                    "type": "TEXT_BLOCK",
                                    "loc": loc,
                                    "table_id": None,
                                    "row_index": None,
                                    "content": content
                                }
                                out.write(json.dumps(rec, ensure_ascii=False) + "\n")
                                doc_id += 1
                                total += 1
                            else:
                                table_id = f"XLSX_{sheet}"
                                for r_i, row in enumerate(s["rows"], start=1):
                                    values = [clean(x) for x in row]
                                    content = canonical_table_row(file_name, "xlsx", loc, table_id, r_i, headers, values)
                                    rec = {
                                        "doc_id": doc_id,
                                        "company": company,
                                        "client_id": client_id,
                                        "file": file_name,
                                        "filetype": "xlsx",
                                        "type": "TABLE_ROW",
                                        "loc": loc,
                                        "table_id": table_id,
                                        "row_index": r_i,
                                        "content": content
                                    }
                                    out.write(json.dumps(rec, ensure_ascii=False) + "\n")
                                    doc_id += 1
                                    total += 1

    print(f"[build_evidence] wrote {total} evidence units -> {evidence_path}")


if __name__ == "__main__":
    main()


# ragservice.py
import json
import os
from pathlib import Path

import boto3
import faiss
import numpy as np
import pandas as pd


def load_config(path: str = "config.json") -> dict:
    return json.loads(Path(path).read_text(encoding="utf-8"))


def allowed_company(company: str, include, exclude) -> bool:
    include = include or []
    exclude = exclude or []
    if include and company not in include:
        return False
    if exclude and company in exclude:
        return False
    return True


def load_evidence(path: Path):
    items = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            items.append(json.loads(line))
    items.sort(key=lambda x: x["doc_id"])
    return items


def embed_titan(bedrock, model_id: str, text: str) -> np.ndarray:
    resp = bedrock.invoke_model(modelId=model_id, body=json.dumps({"inputText": text}))
    vec = json.loads(resp["body"].read())["embedding"]
    v = np.array(vec, dtype="float32")
    n = np.linalg.norm(v)
    v = (v / n) if n > 0 else v
    return v.reshape(1, -1)


def stable_sort_key(it):
    # deterministic ordering so prompts are stable
    type_rank = 0 if it["type"] == "TABLE_ROW" else 1
    file_name = it.get("file", "")
    loc = it.get("loc", "")
    table_id = it.get("table_id") or ""
    row = it.get("row_index") if it.get("row_index") is not None else 10**9
    return (type_rank, file_name, loc, table_id, row, it["doc_id"])


def pick_evidence(candidates, max_units, max_table, max_text):
    candidates.sort(key=stable_sort_key)
    picked, t_count, x_count = [], 0, 0

    for it in candidates:
        if it["type"] == "TABLE_ROW":
            if t_count >= max_table:
                continue
            t_count += 1
        else:
            if x_count >= max_text:
                continue
            x_count += 1

        picked.append(it)
        if len(picked) >= max_units:
            break

    return picked


def extract_text_from_bedrock_anthropic(raw: dict) -> str:
    # handle common Bedrock shapes for Anthropic models
    if isinstance(raw, dict):
        if "content" in raw and isinstance(raw["content"], list) and raw["content"]:
            first = raw["content"][0]
            if isinstance(first, dict) and "text" in first:
                return first["text"]

        out = raw.get("output", {})
        msg = out.get("message", {})
        cont = msg.get("content", [])
        if isinstance(cont, list) and cont and isinstance(cont[0], dict) and "text" in cont[0]:
            return cont[0]["text"]

    return json.dumps(raw)


def call_claude(bedrock, model_id: str, prompt: str) -> str:
    body = {
        "temperature": 0,
        "top_p": 1,
        "max_tokens": 1600,
        "messages": [{"role": "user", "content": prompt}],
    }
    resp = bedrock.invoke_model(modelId=model_id, body=json.dumps(body))
    raw = json.loads(resp["body"].read())
    return extract_text_from_bedrock_anthropic(raw)


def build_prompt(company: str, client_id: str, fields, evidence_units):
    blocks = []
    for i, it in enumerate(evidence_units, start=1):
        ref = f"EVID_{i}"
        meta = {
            "ref": ref,
            "company": it.get("company"),
            "client": it.get("client_id"),
            "file": it.get("file"),
            "filetype": it.get("filetype"),
            "type": it.get("type"),
            "loc": it.get("loc"),
            "table_id": it.get("table_id"),
            "row_index": it.get("row_index"),
        }
        blocks.append(f"[{ref}]\nMETA: {json.dumps(meta)}\nCONTENT:\n{it['content']}")

    evidence_text = "\n\n".join(blocks)

    rules = """
You are a strict information extraction engine.

Rules:
1) Use ONLY the provided evidence.
2) Return ONLY a valid JSON object. No markdown. No commentary. No extra keys.
3) ONLY include fields that you can find verbatim in the evidence.
4) If a field is not found verbatim, OMIT it entirely (do NOT output NOT_FOUND / null / empty).
5) Values must be copied EXACTLY as written (character-for-character).
6) Do not calculate, infer, normalize, or combine values.

Output format:
{
  "Company": "<company>",
  "Client": "<client>",
  "fields": {
    "<Field Name>": "<verbatim value>",
    ...
  }
}
""".strip()

    user = (
        f"Company: {company}\n"
        f"Client: {client_id}\n"
        "Fields:\n- " + "\n- ".join(fields) + "\n\n"
        f"EVIDENCE:\n{evidence_text}"
    )

    return rules + "\n\n" + user, evidence_text


def validate_and_prune(fields_dict, evidence_blob):
    # keep only values that appear verbatim in evidence
    cleaned = {}
    for k, v in (fields_dict or {}).items():
        if isinstance(v, str) and v and (v in evidence_blob):
            cleaned[k] = v
    return cleaned


def main():
    cfg = load_config()

    region = cfg.get("aws_region", "us-east-1")
    embed_model_id = os.getenv("BEDROCK_EMBED_MODEL_ID")
    llm_model_id = os.getenv("BEDROCK_LLM_MODEL_ID")

    if not embed_model_id or not llm_model_id:
        raise RuntimeError(
            "Set BEDROCK_EMBED_MODEL_ID and BEDROCK_LLM_MODEL_ID as environment variables."
        )

    evidence_path = Path(cfg["evidence_jsonl_path"])
    faiss_path = cfg["faiss_index_path"]
    excel_path = Path(cfg["excel_path"])

    include = cfg.get("include_companies", [])
    exclude = cfg.get("exclude_companies", [])

    top_k = int(cfg["top_k_retrieve"])
    max_units = int(cfg["max_evidence_units"])
    max_table = int(cfg["max_table_units"])
    max_text = int(cfg["max_text_units"])

    session = boto3.Session(region_name=region)
    bedrock = session.client("bedrock-runtime")

    evidence_items = load_evidence(evidence_path)
    index = faiss.read_index(faiss_path)

    fields = [
        "airlines included in the contract",
        "OSI",
        "TKT Designator",
        "TOUR CODE",
        "Contact",
        "Global Account Manager",
        "contract Expiry date",
        "GDS fare loading codes",
        "Air corporate Loyality Program",
        "Perks ID",
        "Point of sale validity which market this deal can be sold from?",
        "fare/route details and ticketing instruction received",
        "status",
        "conteact expire",
    ]

    # Build list of (company, client) pairs from evidence
    pairs = set()
    for it in evidence_items:
        c = it.get("company")
        cl = it.get("client_id")
        if c and cl and allowed_company(c, include, exclude):
            pairs.add((c, cl))

    pairs = sorted(pairs, key=lambda x: (x[0], x[1]))
    if not pairs:
        print("No (company, client) pairs found in evidence.jsonl. Did build_evidence.py run?")
        return

    rows = []
    query = " | ".join(fields)
    qvec = embed_titan(bedrock, embed_model_id, query)

    for company, client_id in pairs:
        scores, ids = index.search(qvec, top_k)

        # Collect candidate evidence for this (company, client)
        candidates = []
        for doc_id in ids[0].tolist():
            if doc_id < 0:
                continue
            it = evidence_items[doc_id]
            if it.get("company") == company and it.get("client_id") == client_id:
                candidates.append(it)

        picked = pick_evidence(candidates, max_units, max_table, max_text)

        # If nothing retrieved for this pair, write blanks
        if not picked:
            row = {"Company": company, "Client": client_id}
            for f in fields:
                row[f] = ""
            rows.append(row)
            continue

        prompt, evidence_blob = build_prompt(company, client_id, fields, picked)
        txt = call_claude(bedrock, llm_model_id, prompt)

        try:
            parsed = json.loads(txt)
        except Exception:
            parsed = {"Company": company, "Client": client_id, "fields": {}}

        found = validate_and_prune(parsed.get("fields", {}), evidence_blob)

        row = {"Company": company, "Client": client_id}
        for f in fields:
            row[f] = found.get(f, "")
        rows.append(row)

    excel_path.parent.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(rows).to_excel(excel_path, index=False)
    print(f"[ragservice] saved -> {excel_path}")


if __name__ == "__main__":
    main()



                                              
