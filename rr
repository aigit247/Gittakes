# ============================
# ragservice in ONE Jupyter cell (per-client FAISS + deterministic + TEXT-first then TABLE-block fallback)
#
# Requirements:
# - config.json with at least:
#     {
#       "aws_region": "us-east-1",
#       "faiss_index_root": "index/by_client",
#       "excel_path": "out/results.xlsx",
#       "top_each_per_field": 25,
#       "top_k_retrieve": 250,
#       "max_evidence_units": 90,
#       "max_table_units": 60,
#       "max_text_units": 30
#     }
# - env vars:
#     BEDROCK_EMBED_MODEL_ID
#     BEDROCK_LLM_MODEL_ID
#
# Notes:
# - Loads per-client stores:
#     <faiss_index_root>/<company>/<client_id>/evidence.jsonl
#     <faiss_index_root>/<company>/<client_id>/faiss.index
# - PASS 1: uses NON-table items only (avoids tables)
# - PASS 2: only for missing fields, uses TABLE_ROW collapsed into TABLE_BLOCK (whole tables) and sends only tables
# ============================

import json, os, re
from pathlib import Path
from collections import defaultdict
from typing import Any, Dict, List, Tuple

import boto3
import faiss
import numpy as np
import pandas as pd


# ----------------- config -----------------
cfg = json.loads(Path("config.json").read_text(encoding="utf-8"))
AWS_REGION = cfg.get("aws_region", "us-east-1")
FAISS_ROOT = Path(cfg.get("faiss_index_root", "index/by_client"))
EXCEL_PATH = Path(cfg.get("excel_path", "out/results.xlsx"))

INCLUDE = cfg.get("include_companies", []) or []
EXCLUDE = cfg.get("exclude_companies", []) or []

TOP_EACH = int(cfg.get("top_each_per_field", 25))
TOP_GLOBAL = int(cfg.get("top_k_retrieve", 250))

MAX_UNITS = int(cfg.get("max_evidence_units", 90))
MAX_TABLE = int(cfg.get("max_table_units", 60))
MAX_TEXT = int(cfg.get("max_text_units", 30))

# Table-block safety caps (still "table blocks", never row-wise in prompts)
TABLE_BLOCK_MAX_ROWS = cfg.get("table_block_max_rows", None)      # None => keep whole table if possible
TABLE_BLOCK_MAX_CHARS = int(cfg.get("table_block_max_chars", 25000))

# Choose which to run
RUN_ALL_PAIRS = bool(cfg.get("run_all_pairs", True))
TEST_COMPANY = cfg.get("test_company", "")  # if RUN_ALL_PAIRS=False, set these
TEST_CLIENT  = cfg.get("test_client", "")

# ----------------- models -----------------
BEDROCK_EMBED_MODEL_ID = os.getenv("BEDROCK_EMBED_MODEL_ID")
BEDROCK_LLM_MODEL_ID   = os.getenv("BEDROCK_LLM_MODEL_ID")
if not BEDROCK_EMBED_MODEL_ID or not BEDROCK_LLM_MODEL_ID:
    raise RuntimeError("Set env vars: BEDROCK_EMBED_MODEL_ID and BEDROCK_LLM_MODEL_ID")

session = boto3.Session(region_name=AWS_REGION)
bedrock = session.client("bedrock-runtime")


# ----------------- fields -----------------
FIELDS = [
    "airlines included in the contract",
    "OSI",
    "TKT Designator",
    "TOUR CODE",
    "Contact",
    "Global Account Manager",
    "contract Expiry date",
    "GDS fare loading codes",
    "Air corporate Loyality Program",
    "Perks ID",
    "Point of sale validity which market this deal can be sold from?",
    "fare/route details and ticketing instruction received",
    "status",
    "conteact expire",
]

FIELD_SYNONYMS = {
    "contract Expiry date": ["expiration date", "expiry date", "valid until", "validity", "expires"],
    "TOUR CODE": ["tourcode", "tour code", "tour-code"],
    "TKT Designator": ["ticket designator", "tkt designator"],
    "OSI": ["other service information", "OSI"],
    "GDS fare loading codes": ["fare loading code", "GDS code", "loading code"],
    "Air corporate Loyality Program": ["corporate loyalty program", "loyalty program"],
    "Contact": ["point of contact", "POC", "contact"],
    "status": ["current status", "status"],
    "fare/route details and ticketing instruction received": ["ticketing instruction", "ticketing instructions", "route details", "fare details"],
}


# ----------------- helpers -----------------
def allowed_company(company: str) -> bool:
    if INCLUDE and company not in INCLUDE:
        return False
    if EXCLUDE and company in EXCLUDE:
        return False
    return True

def normalize(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def prune_fields(fields_dict: dict, evidence_content_blob: str) -> dict:
    ev = normalize(evidence_content_blob)
    cleaned = {}
    for k, v in (fields_dict or {}).items():
        if isinstance(v, str) and v.strip():
            if normalize(v) in ev:
                cleaned[k] = v
    return cleaned

def safe_json_load(text: str) -> dict:
    text = (text or "").strip()
    m = re.search(r"\{.*\}", text, re.S)
    if m:
        text = m.group(0)
    return json.loads(text)

def embed_titan(text: str) -> np.ndarray:
    resp = bedrock.invoke_model(
        modelId=BEDROCK_EMBED_MODEL_ID,
        body=json.dumps({"inputText": text}),
        accept="application/json",
        contentType="application/json",
    )
    vec = json.loads(resp["body"].read())["embedding"]
    v = np.array(vec, dtype="float32")
    n = np.linalg.norm(v)
    if n > 0:
        v = v / n
    return v.reshape(1, -1)

def extract_text_any(raw: dict) -> str:
    if isinstance(raw, dict):
        if "content" in raw and isinstance(raw["content"], list) and raw["content"]:
            first = raw["content"][0]
            if isinstance(first, dict) and "text" in first:
                return first["text"]
        out = raw.get("output", {})
        msg = out.get("message", {})
        cont = msg.get("content", [])
        if isinstance(cont, list) and cont and isinstance(cont[0], dict) and "text" in cont[0]:
            return cont[0]["text"]
        if "completion" in raw and isinstance(raw["completion"], str):
            return raw["completion"]
    return ""

def call_claude(prompt: str) -> str:
    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "temperature": 0,
        "max_tokens": 1600,
        "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
    }
    resp = bedrock.invoke_model(
        modelId=BEDROCK_LLM_MODEL_ID,
        body=json.dumps(body),
        accept="application/json",
        contentType="application/json",
    )
    raw_text = resp["body"].read().decode("utf-8", errors="replace")
    raw_json = json.loads(raw_text)
    return extract_text_any(raw_json)

def expand_retrieval_queries(fields: List[str]) -> List[str]:
    queries = []
    for f in fields:
        queries.append(f)
        for syn in FIELD_SYNONYMS.get(f, []):
            queries.append(syn)
    return queries

def load_pair_store(company: str, client_id: str) -> Tuple[List[Dict[str, Any]], faiss.Index]:
    pair_dir = FAISS_ROOT / company / client_id
    evidence_path = pair_dir / "evidence.jsonl"
    index_path = pair_dir / "faiss.index"
    if not evidence_path.exists() or not index_path.exists():
        raise FileNotFoundError(f"Missing store: {pair_dir}")

    # IMPORTANT: do NOT sort; order must align with FAISS ids
    items = []
    with open(evidence_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                items.append(json.loads(line))

    idx = faiss.read_index(str(index_path))
    if idx.ntotal != len(items):
        raise RuntimeError(f"FAISS ntotal ({idx.ntotal}) != evidence rows ({len(items)}) for {company}/{client_id}")

    return items, idx

def list_pairs_from_root() -> List[Tuple[str, str]]:
    manifest = FAISS_ROOT / "manifest.json"
    if manifest.exists():
        m = json.loads(manifest.read_text(encoding="utf-8"))
        pairs = []
        for p in m.get("pairs", []) or []:
            c = p.get("company"); cl = p.get("client_id")
            if c and cl:
                pairs.append((c, cl))
        pairs = sorted(pairs, key=lambda x: (x[0], x[1]))
        return [pc for pc in pairs if allowed_company(pc[0])]

    # fallback scan
    pairs = []
    for cdir in FAISS_ROOT.iterdir():
        if not cdir.is_dir() or cdir.name.startswith("_"):
            continue
        if not allowed_company(cdir.name):
            continue
        for cldir in cdir.iterdir():
            if not cldir.is_dir():
                continue
            if (cldir / "faiss.index").exists() and (cldir / "evidence.jsonl").exists():
                pairs.append((cdir.name, cldir.name))
    return sorted(pairs, key=lambda x: (x[0], x[1]))

def retrieve_union_for_queries(
    evidence_items: List[Dict[str, Any]],
    faiss_index: faiss.Index,
    queries: List[str],
    top_each: int,
    top_global: int
) -> List[Dict[str, Any]]:
    all_ids = set()
    best_score: Dict[int, float] = {}

    for q in queries:
        qvec = embed_titan(q)
        scores, ids = faiss_index.search(qvec, top_each)
        for s, doc_id in zip(scores[0].tolist(), ids[0].tolist()):
            if doc_id < 0:
                continue
            all_ids.add(doc_id)
            if doc_id not in best_score or float(s) > best_score[doc_id]:
                best_score[doc_id] = float(s)

    global_q = " | ".join(queries[: min(len(queries), 60)])
    qvec = embed_titan(global_q)
    scores, ids = faiss_index.search(qvec, top_global)
    for s, doc_id in zip(scores[0].tolist(), ids[0].tolist()):
        if doc_id < 0:
            continue
        all_ids.add(doc_id)
        if doc_id not in best_score or float(s) > best_score[doc_id]:
            best_score[doc_id] = float(s)

    out = []
    for i in sorted(all_ids):
        it = dict(evidence_items[i])
        it["_score"] = best_score.get(i, 0.0)
        out.append(it)
    return out

def stable_sort_key(it: Dict[str, Any]):
    # score-first deterministic, then stable metadata
    score = float(it.get("_score", 0.0))
    t = it.get("type", "")
    type_rank = 0 if t in ("TABLE_ROW", "TABLE_BLOCK") else 1
    file_name = it.get("file", "")
    loc = it.get("loc", "")
    table_id = it.get("table_id") or ""
    row = it.get("row_index") if it.get("row_index") is not None else 10**9
    return (-score, type_rank, file_name, loc, table_id, row, it.get("doc_id", 0))

def pick_evidence(items: List[Dict[str, Any]], max_units: int, max_table: int, max_text: int,
                 allow_tables: bool = True, allow_text: bool = True) -> List[Dict[str, Any]]:
    items = list(items)
    items.sort(key=stable_sort_key)

    picked, t_count, x_count = [], 0, 0
    for it in items:
        is_table = it.get("type") in ("TABLE_ROW", "TABLE_BLOCK")
        if is_table:
            if not allow_tables:
                continue
            if t_count >= max_table:
                continue
            t_count += 1
        else:
            if not allow_text:
                continue
            if x_count >= max_text:
                continue
            x_count += 1

        picked.append(it)
        if len(picked) >= max_units:
            break
    return picked

# ---- Collapse TABLE_ROW -> TABLE_BLOCK (whole table in prompt, deterministic) ----
def _parse_headers_values(row_text: str) -> Tuple[str, str]:
    headers = ""
    values = ""
    for ln in (row_text or "").splitlines():
        s = ln.strip()
        if s.upper().startswith("HEADERS:"):
            headers = s.split(":", 1)[1].strip()
        elif s.upper().startswith("VALUES:"):
            values = s.split(":", 1)[1].strip()
    return headers, values

def collapse_table_rows_to_blocks(items: List[Dict[str, Any]],
                                  max_rows_per_block=None,
                                  max_chars_per_block=25000) -> List[Dict[str, Any]]:
    non_table = [it for it in items if it.get("type") != "TABLE_ROW"]
    rows = [it for it in items if it.get("type") == "TABLE_ROW"]

    groups = defaultdict(list)
    def group_key(it):
        file = it.get("file", "") or ""
        table_id = it.get("table_id") or ""
        loc = it.get("loc", "") or ""
        loc_prefix = str(loc).split("ROW=")[0].strip()
        return (file, table_id, loc_prefix)

    for r in rows:
        groups[group_key(r)].append(r)

    blocks = []
    for (file, table_id, loc_prefix), rs in sorted(groups.items(), key=lambda x: (x[0][0], x[0][1], x[0][2])):
        rs.sort(key=lambda x: (x.get("row_index") if x.get("row_index") is not None else 10**9, x.get("doc_id", 0)))
        max_score = max((float(r.get("_score", 0.0)) for r in rs), default=0.0)

        hdr = ""
        for r in rs:
            h, _ = _parse_headers_values(r.get("content", ""))
            if h:
                hdr = h
                break
        hdr = hdr or "(unknown headers)"

        def make_block(chunk_rows: List[Dict[str, Any]], chunk_idx: int) -> Dict[str, Any]:
            vals = []
            for rr in chunk_rows:
                _, v = _parse_headers_values(rr.get("content", ""))
                vals.append(v if v else (rr.get("content", "") or "").strip())

            r0 = chunk_rows[0].get("row_index")
            r1 = chunk_rows[-1].get("row_index")

            content = (
                f"TYPE=TABLE_BLOCK\n"
                f"FILE={file}\n"
                f"TABLE_ID={table_id}\n"
                f"LOC={loc_prefix}\n"
                f"ROW_RANGE={r0}-{r1}\n"
                f"HEADERS: {hdr}\n"
                f"ROWS:\n- " + "\n- ".join(vals)
            )

            blk = {
                "company": chunk_rows[0].get("company"),
                "client_id": chunk_rows[0].get("client_id"),
                "file": file,
                "filetype": chunk_rows[0].get("filetype"),
                "type": "TABLE_BLOCK",
                "loc": loc_prefix if chunk_idx == 1 else f"{loc_prefix} CHUNK={chunk_idx}",
                "table_id": table_id,
                "row_index": r0,
                "doc_id": chunk_rows[0].get("doc_id", 0),
                "_score": max_score,
                "content": content,
            }
            return blk

        # keep whole table unless too large
        if max_rows_per_block is None:
            blk = make_block(rs, 1)
            if len(blk["content"]) <= max_chars_per_block:
                blocks.append(blk)
            else:
                chunk_size = 300
                chunk_idx = 1
                for i in range(0, len(rs), chunk_size):
                    blocks.append(make_block(rs[i:i+chunk_size], chunk_idx))
                    chunk_idx += 1
        else:
            chunk_size = int(max_rows_per_block)
            chunk_idx = 1
            for i in range(0, len(rs), chunk_size):
                blk = make_block(rs[i:i+chunk_size], chunk_idx)
                if len(blk["content"]) > max_chars_per_block and len(rs[i:i+chunk_size]) > 50:
                    sub = rs[i:i+chunk_size]
                    sub_size = 150
                    sub_idx = 1
                    for j in range(0, len(sub), sub_size):
                        blocks.append(make_block(sub[j:j+sub_size], chunk_idx*100 + sub_idx))
                        sub_idx += 1
                else:
                    blocks.append(blk)
                chunk_idx += 1

    out = non_table + blocks
    out.sort(key=stable_sort_key)
    return out

def build_prompt(company: str, client_id: str, fields: List[str], evidence_units: List[Dict[str, Any]]):
    # For prompt: include META wrappers
    blocks = []
    for i, it in enumerate(evidence_units, start=1):
        ref = f"EVID_{i}"
        meta = {
            "ref": ref,
            "company": it.get("company"),
            "client": it.get("client_id"),
            "file": it.get("file"),
            "filetype": it.get("filetype"),
            "type": it.get("type"),
            "loc": it.get("loc"),
            "table_id": it.get("table_id"),
            "row_index": it.get("row_index"),
            "score": it.get("_score", 0.0),
        }
        blocks.append(f"[{ref}]\nMETA: {json.dumps(meta)}\nCONTENT:\n{it.get('content','')}")

    evidence_text = "\n\n".join(blocks)
    evidence_content_blob = "\n\n".join([it.get("content", "") or "" for it in evidence_units])  # for pruning

    prompt = (
        "You are a strict extraction engine.\n\n"
        "Rules:\n"
        "1) Use ONLY the provided evidence.\n"
        "2) Return ONLY a valid JSON object. No markdown. No commentary.\n"
        "3) ONLY include fields where the VALUE can be copied verbatim from evidence.\n"
        "4) If not found, OMIT the field.\n"
        "5) Do not infer, normalize, or compute.\n\n"
        "Output format:\n"
        "{\n"
        f'  "Company": "{company}",\n'
        f'  "Client": "{client_id}",\n'
        '  "fields": {\n'
        '    "<Field Name>": "<verbatim value>"\n'
        "  }\n"
        "}\n\n"
        "Fields:\n- " + "\n- ".join(fields) + "\n\n"
        "EVIDENCE:\n" + evidence_text
    )
    return prompt, evidence_content_blob


# ----------------- run -----------------
pairs = list_pairs_from_root()
if not pairs:
    raise RuntimeError(f"No pairs found under {FAISS_ROOT}")

if RUN_ALL_PAIRS:
    run_pairs = pairs
else:
    if not TEST_COMPANY or not TEST_CLIENT:
        raise RuntimeError("Set cfg test_company and test_client (or set run_all_pairs=true)")
    run_pairs = [(TEST_COMPANY, TEST_CLIENT)]

print("[dbg] pairs to run:", len(run_pairs))
print("[dbg] first 10:", run_pairs[:10])

rows_out = []

for company, client_id in run_pairs:
    print(f"\n[rag] processing {company}/{client_id}")

    try:
        evidence_items, faiss_index = load_pair_store(company, client_id)
    except Exception as e:
        print("  [warn] load_pair_store failed:", e)
        row = {"Company": company, "Client": client_id}
        for f in FIELDS:
            row[f] = ""
        rows_out.append(row)
        continue

    # ---------- PASS 1: TEXT ONLY ----------
    queries1 = expand_retrieval_queries(FIELDS)
    cand1 = retrieve_union_for_queries(evidence_items, faiss_index, queries1, TOP_EACH, TOP_GLOBAL)
    cand1_text = [it for it in cand1 if it.get("type") not in ("TABLE_ROW", "TABLE_BLOCK")]
    picked1 = pick_evidence(cand1_text, max_units=MAX_UNITS, max_table=0, max_text=max(MAX_TEXT, MAX_UNITS),
                            allow_tables=False, allow_text=True)

    if not picked1:
        pruned1 = {}
        missing = list(FIELDS)
    else:
        prompt1, blob1 = build_prompt(company, client_id, FIELDS, picked1)
        try:
            out1 = call_claude(prompt1)
            parsed1 = safe_json_load(out1)
            raw_fields_1 = parsed1.get("fields", {}) if isinstance(parsed1, dict) else {}
            pruned1 = prune_fields(raw_fields_1, blob1)
        except Exception as e:
            print("  [warn] PASS1 failed:", e)
            pruned1 = {}

        missing = [f for f in FIELDS if not pruned1.get(f)]

    # ---------- PASS 2: TABLE BLOCKS ONLY (for missing) ----------
    final_fields = dict(pruned1)

    if missing:
        queries2 = expand_retrieval_queries(missing)
        cand2 = retrieve_union_for_queries(evidence_items, faiss_index, queries2, TOP_EACH, TOP_GLOBAL)
        # collapse TABLE_ROW to TABLE_BLOCK and keep only tables
        cand2_blocks = collapse_table_rows_to_blocks(cand2, max_rows_per_block=TABLE_BLOCK_MAX_ROWS, max_chars_per_block=TABLE_BLOCK_MAX_CHARS)
        cand2_tables = [it for it in cand2_blocks if it.get("type") == "TABLE_BLOCK"]

        picked2 = pick_evidence(cand2_tables, max_units=MAX_UNITS, max_table=MAX_TABLE, max_text=0,
                                allow_tables=True, allow_text=False)

        if picked2:
            prompt2, blob2 = build_prompt(company, client_id, missing, picked2)
            try:
                out2 = call_claude(prompt2)
                parsed2 = safe_json_load(out2)
                raw_fields_2 = parsed2.get("fields", {}) if isinstance(parsed2, dict) else {}
                pruned2 = prune_fields(raw_fields_2, blob2)
                for f in missing:
                    if pruned2.get(f):
                        final_fields[f] = pruned2[f]
            except Exception as e:
                print("  [warn] PASS2 failed:", e)

    # ---------- row output ----------
    row = {"Company": company, "Client": client_id}
    for f in FIELDS:
        row[f] = final_fields.get(f, "")
    rows_out.append(row)

# save excel
EXCEL_PATH.parent.mkdir(parents=True, exist_ok=True)
pd.DataFrame(rows_out).to_excel(EXCEL_PATH, index=False)
print(f"\n[rag] saved -> {EXCEL_PATH.resolve()}")
