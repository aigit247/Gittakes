# indexer.py
from __future__ import annotations

import argparse
import io
import json
import os
import re
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import boto3
import fitz  # PyMuPDF
import pandas as pd
from langchain.schema import Document
from langchain_aws import BedrockEmbeddings
from langchain_community.document_loaders import AmazonTextractPDFLoader
from langchain_community.vectorstores import FAISS

# =============================================================================
# CONFIG
# =============================================================================

LOCAL_DOC_ROOT = Path(os.getenv("LOCAL_DOC_ROOT", "/tmp/docs")).resolve()
FAISS_INDEX_DIR = Path(os.getenv("FAISS_INDEX_DIR", "/tmp/faiss_indexes")).resolve()

S3_BUCKET = os.getenv("S3_BUCKET", "").strip()
S3_DOC_PREFIX = os.getenv("S3_DOC_PREFIX", "").strip()  # maps LOCAL_DOC_ROOT -> S3 prefix, e.g. "user/d/ce"

# ===================== AWS / BEDROCK / TEXTRACT CONFIG =====================

REGION = os.getenv("AWS_REGION", "us-east-1")
EMBED_MODEL = os.getenv("BEDROCK_EMBED_MODEL", "amazon.titan-embed-text-v2:0")

# ===================== Separate Sessions: S3/Textract vs Bedrock =====================
# S3/Textract credentials
S3_AWS_ACCESS_KEY_ID = os.getenv("S3_AWS_ACCESS_KEY_ID") or os.getenv("AWS_ACCESS_KEY_ID")
S3_AWS_SECRET_ACCESS_KEY = os.getenv("S3_AWS_SECRET_ACCESS_KEY") or os.getenv("AWS_SECRET_ACCESS_KEY")

# Bedrock credentials
BEDROCK_AWS_ACCESS_KEY_ID = os.getenv("BEDROCK_AWS_ACCESS_KEY_ID")
BEDROCK_AWS_SECRET_ACCESS_KEY = os.getenv("BEDROCK_AWS_SECRET_ACCESS_KEY")

def _mk_session(access_key_id: Optional[str], secret_access_key: Optional[str]) -> boto3.Session:
    if access_key_id and secret_access_key:
        return boto3.Session(
            aws_access_key_id=access_key_id,
            aws_secret_access_key=secret_access_key,
            region_name=REGION,
        )
    return boto3.Session(region_name=REGION)

S3_SESSION = _mk_session(S3_AWS_ACCESS_KEY_ID, S3_AWS_SECRET_ACCESS_KEY)
BEDROCK_SESSION = _mk_session(BEDROCK_AWS_ACCESS_KEY_ID, BEDROCK_AWS_SECRET_ACCESS_KEY)

TEXTRACT_CLIENT = S3_SESSION.client("textract")
BEDROCK_RUNTIME = BEDROCK_SESSION.client("bedrock-runtime")

CLIENT_GROUP_XLSX_PATH = (os.getenv("CLIENT_GROUP_XLSX_PATH") or "").strip()


# ===================== Metadata helpers (for manifest) =====================
def load_metadata_for_file(doc_path: Path) -> Optional[dict]:
    """
    Looks for sidecar metadata files:
      <file>.metadata.json
      <file>.metadata
    Returns metadataAttributes/metadataAtrributes block if present, else whole JSON.
    """
    meta_candidates = [
        doc_path.with_name(doc_path.name + ".metadata.json"),
        doc_path.with_name(doc_path.name + ".metadata"),
    ]

    for meta_path in meta_candidates:
        if not meta_path.exists():
            continue

        try:
            text = meta_path.read_text(encoding="utf-8", errors="ignore").strip()
            data = json.loads(text)
        except Exception:
            continue

        meta_block = (
            data.get("metadataAtrributes")
            or data.get("metadataAttributes")
            or data
        )
        return meta_block

    return None


@dataclass
class DocInfo:
    policy_id: str
    source_file: str
    pages: int
    modified_time: int
    metadata: Optional[dict] = None


def build_manifest_entry(path: Path) -> DocInfo:
    pages = 0
    if path.suffix.lower() == ".pdf":
        try:
            with fitz.open(str(path)) as doc:
                pages = doc.page_count
        except Exception:
            pages = 0

    stat = path.stat()
    meta = load_metadata_for_file(path)

    return DocInfo(
        policy_id=path.name,
        source_file=str(path.resolve()),
        pages=pages,
        modified_time=int(stat.st_mtime),
        metadata=meta,
    )


# =============================================================================
# HELPERS
# =============================================================================

def _safe_slug(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^A-Za-z0-9_\-\.]", "", s)
    return s or "unknown"


def _rel_to_root(p: Path) -> Path:
    try:
        return p.resolve().relative_to(LOCAL_DOC_ROOT)
    except Exception:
        return Path(p.name)


def _s3_key_for_local(p: Path) -> str:
    rel = _rel_to_root(p).as_posix()
    if S3_DOC_PREFIX:
        return f"{S3_DOC_PREFIX.rstrip('/')}/{rel}"
    return rel


# =============================================================================
# CLIENT GROUP MAPPING (XLSX)
# =============================================================================

@dataclass(frozen=True)
class MappingRow:
    client_folder: str
    scope_input: str  # e.g. "US", "Ultimate Parent", "Corporate Family"
    scope_dir: str    # e.g. "us", "ultimate_parent", "corporate_family"
    group_id: str     # e.g. "group_001"


class ClientGroupMapper:
    def __init__(self, xlsx_path: str):
        self.xlsx_path = xlsx_path
        self.rows: List[MappingRow] = []

    def load(self, s3_client) -> None:
        """
        Loads the mapping xlsx. Supports:
          - local file path
          - s3://bucket/key if provided via CLIENT_GROUP_XLSX_PATH
        """
        if self.xlsx_path.lower().startswith("s3://"):
            m = re.match(r"^s3://([^/]+)/(.+)$", self.xlsx_path, flags=re.I)
            if not m:
                raise ValueError(f"Invalid s3 path: {self.xlsx_path}")
            bucket, key = m.group(1), m.group(2)
            obj = s3_client.get_object(Bucket=bucket, Key=key)
            data = obj["Body"].read()
            df = pd.read_excel(io.BytesIO(data))
        else:
            df = pd.read_excel(self.xlsx_path)

        # Expect columns: client_folder, scope_input, scope_dir, group_id (case-insensitive)
        cols = {c.lower().strip(): c for c in df.columns}
        required = ["client_folder", "scope_input", "scope_dir", "group_id"]
        missing = [c for c in required if c not in cols]
        if missing:
            raise ValueError(f"Mapping sheet missing required columns: {missing}. Found={list(df.columns)}")

        for _, r in df.iterrows():
            client_folder = str(r[cols["client_folder"]]).strip()
            scope_input = str(r[cols["scope_input"]]).strip()
            scope_dir = str(r[cols["scope_dir"]]).strip()
            group_id = str(r[cols["group_id"]]).strip()
            if not client_folder or not scope_input or not scope_dir or not group_id:
                continue
            self.rows.append(MappingRow(client_folder, scope_input, scope_dir, group_id))

    def resolve_group_row(self, client_folder: str, scope_input: str) -> Optional[MappingRow]:
        cf = (client_folder or "").strip()
        sc = (scope_input or "").strip()
        for r in self.rows:
            if r.client_folder == cf and r.scope_input == sc:
                return r
        return None


# =============================================================================
# TEXTRACT / CHUNKING
# =============================================================================

def _textract_loader_for_s3(bucket: str, key: str) -> AmazonTextractPDFLoader:
    # AmazonTextractPDFLoader supports s3:// style if provided, but we keep as explicit for clarity
    uri = f"s3://{bucket}/{key}"
    return AmazonTextractPDFLoader(uri, client=TEXTRACT_CLIENT)


def _extract_text_for_pdf(path: Path) -> str:
    """
    If S3_BUCKET is set, runs Textract via S3; else falls back to local PyMuPDF extraction.
    """
    if S3_BUCKET:
        key = _s3_key_for_local(path)
        loader = _textract_loader_for_s3(S3_BUCKET, key)
        pages = loader.load()
        return "\n".join([p.page_content for p in pages if p.page_content])
    else:
        # Local extraction via PyMuPDF
        chunks: List[str] = []
        with fitz.open(str(path)) as doc:
            for p in doc:
                t = p.get_text("text") or ""
                if t.strip():
                    chunks.append(t)
        return "\n".join(chunks)


def _extract_text_for_txt(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return ""


def _extract_text_for_json(path: Path) -> str:
    try:
        obj = json.loads(path.read_text(encoding="utf-8", errors="ignore"))
        return json.dumps(obj, indent=2, ensure_ascii=False)
    except Exception:
        return ""


def _extract_text_for_csv(path: Path) -> str:
    try:
        df = pd.read_csv(path)
        return df.to_csv(index=False)
    except Exception:
        return ""


def _extract_text_for_xlsx(path: Path) -> str:
    try:
        df = pd.read_excel(path)
        return df.to_csv(index=False)
    except Exception:
        return ""


def extract_text(path: Path) -> str:
    ext = path.suffix.lower()
    if ext == ".pdf":
        return _extract_text_for_pdf(path)
    if ext in (".txt", ".md"):
        return _extract_text_for_txt(path)
    if ext == ".json":
        return _extract_text_for_json(path)
    if ext == ".csv":
        return _extract_text_for_csv(path)
    if ext in (".xls", ".xlsx"):
        return _extract_text_for_xlsx(path)
    return ""


def chunk_text(text: str, chunk_size: int = 1200, overlap: int = 150) -> List[str]:
    t = (text or "").strip()
    if not t:
        return []
    chunks: List[str] = []
    i = 0
    while i < len(t):
        end = min(len(t), i + chunk_size)
        chunks.append(t[i:end])
        if end >= len(t):
            break
        i = max(0, end - overlap)
    return chunks


def make_chunks_for_file(path: Path, category: str, client_folder: str, group_id: str, scope_dir: str) -> List[Document]:
    """
    Produces LangChain Documents with metadata used for retrieval and provenance.
    """
    text = extract_text(path)
    parts = chunk_text(text)
    docs: List[Document] = []
    for idx, part in enumerate(parts):
        docs.append(
            Document(
                page_content=part,
                metadata={
                    "category": category,
                    "client_folder": client_folder,
                    "group_id": group_id,
                    "scope_dir": scope_dir,
                    "policy_id": path.name,
                    "source_file": str(path.resolve()),
                    "chunk_index": idx,
                },
            )
        )
    return docs


# =============================================================================
# EMBEDDINGS + FAISS
# =============================================================================

def embedder():
    return BedrockEmbeddings(model_id=EMBED_MODEL, client=BEDROCK_RUNTIME)


# =============================================================================
# INDEXING
# =============================================================================

def discover_files(client_folder: str, category: str, scope_input: str) -> List[Path]:
    """
    Finds files under:
      LOCAL_DOC_ROOT/<category>/<client_folder>/<scope_input>/
    """
    cat = _safe_slug(category)
    cf = _safe_slug(client_folder)
    sc = _safe_slug(scope_input)

    base = LOCAL_DOC_ROOT / cat / cf / sc
    if not base.exists():
        return []

    files: List[Path] = []
    for p in base.rglob("*"):
        if p.is_file() and p.suffix.lower() in (".pdf", ".txt", ".md", ".json", ".csv", ".xls", ".xlsx"):
            # ignore metadata sidecars
            if p.name.endswith(".metadata") or p.name.endswith(".metadata.json"):
                continue
            files.append(p)
    return sorted(files)


def index_task(mapper: ClientGroupMapper, category: str, client_folder: str, scope_input: str, files: List[Path]) -> None:
    row = mapper.resolve_group_row(client_folder, scope_input)
    if not row:
        print(f"[SKIP] No mapping for client='{client_folder}' scope='{scope_input}' category='{category}'")
        return

    group_id = row.group_id
    scope_dir = row.scope_dir

    out_dir = FAISS_INDEX_DIR / category / group_id / scope_dir
    out_dir.mkdir(parents=True, exist_ok=True)

    docs: List[Document] = []
    manifest_rows: List[DocInfo] = []

    for path in files:
        for d in make_chunks_for_file(path, category, client_folder, group_id, scope_dir):
            docs.append(d)
        manifest_rows.append(build_manifest_entry(path))

    if not docs:
        print(f"[{category}/{client_folder}/{scope_dir}] No extractable text; skipping.")
        return

    print(f"[{category}/{client_folder}/{scope_dir}] Building FAISS with {len(docs)} chunks...")
    vs = FAISS.from_documents(docs, embedder())
    vs.save_local(str(out_dir))

    manifest = {
        "category": category,
        "client_folder": client_folder,
        "group_id": group_id,
        "scope_input": scope_input,
        "scope_dir": scope_dir,
        "docs": [
            {
                "policy_id": m.policy_id,
                "source_file": m.source_file,
                "pages": m.pages,
                "modified_time": m.modified_time,
                "metadata": m.metadata,
            }
            for m in manifest_rows
        ],
        "generated_time": int(time.time()),
    }
    (out_dir / "manifest.json").write_text(json.dumps(manifest, indent=2), encoding="utf-8")
    print(f"[{category}/{client_folder}/{scope_dir}] Done. Index at {out_dir}")


def _parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--client-folder", default=os.getenv("TARGET_CLIENT_FOLDER", "").strip())
    p.add_argument("--scope", default=os.getenv("TARGET_SCOPE", "").strip())  # US / Ultimate Parent / Corporate Family
    p.add_argument("--category", default=os.getenv("TARGET_CATEGORY", "").strip().lower())  # legal/finance optional
    return p.parse_args()


def main():
    if not CLIENT_GROUP_XLSX_PATH:
        raise RuntimeError("CLIENT_GROUP_XLSX_PATH is required")

    args = _parse_args()
    if not args.client_folder or not args.scope or not args.category:
        raise RuntimeError("TARGET_CLIENT_FOLDER, TARGET_SCOPE, TARGET_CATEGORY are required (or pass flags).")

    s3 = S3_SESSION.client("s3")
    mapper = ClientGroupMapper(CLIENT_GROUP_XLSX_PATH)
    mapper.load(s3)

    files = discover_files(args.client_folder, args.category, args.scope)
    if not files:
        print(f"[INFO] No files found for category={args.category} client={args.client_folder} scope={args.scope}")
        return

    index_task(mapper, args.category, args.client_folder, args.scope, files)


if __name__ == "__main__":
    main()
