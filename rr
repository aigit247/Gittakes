# Baseline (NO FAISS): TEXT-first extraction, then TABLE fallback (whole-table chunks), deterministic.
# Paste this into ONE Jupyter cell and run. Adjust the CONFIG section.

import os, re, json, time, math
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional

import boto3

# Optional deps (install if missing):
#   pip install python-docx openpyxl pypdf
try:
    import docx  # python-docx
except Exception:
    docx = None

try:
    import openpyxl
except Exception:
    openpyxl = None

try:
    from pypdf import PdfReader
except Exception:
    PdfReader = None


# =========================
# CONFIG (EDIT THESE)
# =========================
COMPANY = "company1"
CLIENT  = "client1"

# Local data folder that contains: data/<Company>/<Client>/*.(pdf|docx|xlsx)
DATA_ROOT = Path("data")

# Textract needs PDFs in S3 for async analysis:
S3_BUCKET = os.environ.get("S3_BUCKET", "")  # <-- set this (recommended) OR export env var
S3_PREFIX = os.environ.get("S3_PREFIX", "data")  # default matches your earlier structure

AWS_REGION = os.environ.get("AWS_REGION", "us-east-1")

# Bedrock Claude model id (set yours). Example (depends on your account access):
BEDROCK_MODEL_ID = os.environ.get("BEDROCK_MODEL_ID", "anthropic.claude-3-5-sonnet-20240620-v1:0")

# Snippet chunking
PDF_TEXT_LINES_PER_SNIPPET = 15
DOCX_PARAS_PER_SNIPPET     = 6
MAX_TABLE_ROWS_PER_CHUNK   = 350  # if a table/sheet is bigger than this, split into big table-block chunks

# Retrieval caps
K_TEXT_PER_FIELD  = 8
K_TABLE_PER_FIELD = 3

# Prompt caps (to avoid blowing context)
MAX_TEXT_SNIPPETS_TOTAL  = 120
MAX_TABLE_SNIPPETS_TOTAL = 35

# Determinism
TEMPERATURE = 0

# Fields + synonyms:
# Option A) Define directly here (recommended).
FIELDS = [

]
FIELD_SYNONYMS = {

}

# Optional: fields that you WANT to prefer text for (tables used only if missing)
PREFER_TEXT_FIELDS = set(FIELDS)  # keep simple: text-first for everything; tables only fill missing


# =========================
# Helpers
# =========================
def norm(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"\s+", " ", s).strip()
    return s

def safe_json_load(s: str) -> Any:
    # Extract first JSON object/array if model adds extra text
    s = s.strip()
    m = re.search(r"(\{.*\}|\[.*\])", s, re.S)
    if m:
        s = m.group(1)
    return json.loads(s)

def chunk_list(items: List[str], n: int) -> List[List[str]]:
    return [items[i:i+n] for i in range(0, len(items), n)]

def tsv_table(rows: List[List[str]]) -> str:
    return "\n".join("\t".join((c or "").strip() for c in r) for r in rows)

def stable_sort_snippets(snips: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    return sorted(snips, key=lambda x: x["snippet_id"])

def score_snippet(field: str, queries: List[str], snippet: Dict[str, Any]) -> float:
    """
    Simple deterministic scoring:
      - phrase hits are worth more than token hits
      - slight preference if field name itself appears
    """
    text = norm(snippet["content"])
    if not text:
        return 0.0

    score = 0.0
    for q in queries:
        qn = norm(q)
        if not qn:
            continue
        if qn in text:
            score += 8.0
        # token hits
        toks = [t for t in re.split(r"[^a-z0-9]+", qn) if len(t) >= 3]
        if toks:
            hit = sum(1 for t in toks if t in text)
            score += 1.0 * hit

    # mild boosts for "value-like" patterns
    if re.search(r"\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b", snippet["content"]):  # date-ish
        score += 0.5
    if re.search(r"[$€£]\s?\d", snippet["content"]) or re.search(r"\b\d{1,3}(,\d{3})+(\.\d+)?\b", snippet["content"]):
        score += 0.5

    return score


# =========================
# Textract (PDF text + tables)
# =========================
def textract_start_and_fetch_all(textract_client, bucket: str, key: str) -> Dict[str, Any]:
    resp = textract_client.start_document_analysis(
        DocumentLocation={"S3Object": {"Bucket": bucket, "Name": key}},
        FeatureTypes=["TABLES"]
    )
    job_id = resp["JobId"]

    # poll
    while True:
        out = textract_client.get_document_analysis(JobId=job_id)
        status = out["JobStatus"]
        if status in ("SUCCEEDED", "FAILED", "PARTIAL_SUCCESS"):
            break
        time.sleep(2)

    if status != "SUCCEEDED":
        raise RuntimeError(f"Textract job {job_id} status={status}")

    # paginate
    blocks = out.get("Blocks", [])
    next_token = out.get("NextToken")
    while next_token:
        page = textract_client.get_document_analysis(JobId=job_id, NextToken=next_token)
        blocks.extend(page.get("Blocks", []))
        next_token = page.get("NextToken")

    return {"Blocks": blocks}

def textract_blocks_to_tables_and_lines(textract_json: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], List[Tuple[int, str]]]:
    blocks = textract_json.get("Blocks", [])
    by_id = {b["Id"]: b for b in blocks if "Id" in b}

    # lines (page, text)
    lines = []
    for b in blocks:
        if b.get("BlockType") == "LINE" and b.get("Text"):
            page = int(b.get("Page", 0) or 0)
            lines.append((page, b["Text"]))

    # tables (each as matrix)
    tables = []
    table_blocks = [b for b in blocks if b.get("BlockType") == "TABLE"]
    for t_i, tb in enumerate(table_blocks, start=1):
        page = int(tb.get("Page", 0) or 0)

        # gather cells
        cell_ids = []
        for rel in tb.get("Relationships", []):
            if rel.get("Type") == "CHILD":
                cell_ids.extend(rel.get("Ids", []))

        cells = []
        max_r = 0
        max_c = 0
        for cid in cell_ids:
            cb = by_id.get(cid)
            if not cb or cb.get("BlockType") != "CELL":
                continue
            r = int(cb.get("RowIndex", 1))
            c = int(cb.get("ColumnIndex", 1))
            max_r = max(max_r, r)
            max_c = max(max_c, c)

            # extract text inside cell
            words = []
            for rel in cb.get("Relationships", []):
                if rel.get("Type") == "CHILD":
                    for wid in rel.get("Ids", []):
                        wb = by_id.get(wid)
                        if not wb:
                            continue
                        if wb.get("BlockType") == "WORD" and wb.get("Text"):
                            words.append(wb["Text"])
                        elif wb.get("BlockType") == "SELECTION_ELEMENT" and wb.get("SelectionStatus") == "SELECTED":
                            words.append("[X]")
            cells.append((r, c, " ".join(words).strip()))

        matrix = [["" for _ in range(max_c)] for __ in range(max_r)]
        for r, c, txt in cells:
            if 1 <= r <= max_r and 1 <= c <= max_c:
                matrix[r-1][c-1] = txt

        tables.append({
            "page": page,
            "table_index": t_i,
            "matrix": matrix
        })

    # Stable ordering
    lines.sort(key=lambda x: (x[0], x[1]))
    tables.sort(key=lambda x: (x["page"], x["table_index"]))
    return tables, lines


# =========================
# Snippet builders (PDF/DOCX/XLSX)
# =========================
def build_pdf_snippets_from_textract(company: str, client: str, source_file: str,
                                     textract_json: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    tables, lines = textract_blocks_to_tables_and_lines(textract_json)

    # Text snippets: chunk by page, then fixed lines per snippet
    lines_by_page: Dict[int, List[str]] = {}
    for p, txt in lines:
        lines_by_page.setdefault(p, []).append(txt)

    text_snips = []
    for p in sorted(lines_by_page.keys()):
        chunks = chunk_list(lines_by_page[p], PDF_TEXT_LINES_PER_SNIPPET)
        for j, ch in enumerate(chunks, start=1):
            content = "\n".join(ch).strip()
            if not content:
                continue
            text_snips.append({
                "snippet_id": f"pdf:{source_file}:p{p}:text{j}",
                "company": company,
                "client": client,
                "source_file": source_file,
                "type": "text",
                "loc": {"page": p, "chunk": j},
                "content": content
            })

    # Table snippets: whole table as TSV; if huge, split into big table-block chunks
    table_snips = []
    for t in tables:
        matrix = t["matrix"]
        if not matrix:
            continue

        if len(matrix) <= MAX_TABLE_ROWS_PER_CHUNK:
            content = tsv_table(matrix)
            table_snips.append({
                "snippet_id": f"pdf:{source_file}:p{t['page']}:table{t['table_index']}",
                "company": company,
                "client": client,
                "source_file": source_file,
                "type": "table",
                "loc": {"page": t["page"], "table": t["table_index"], "row_range": [1, len(matrix)]},
                "content": content
            })
        else:
            # keep header rows (first 1-2) in every chunk for context
            header = matrix[:2]
            body = matrix[2:]
            body_chunks = [body[i:i+MAX_TABLE_ROWS_PER_CHUNK] for i in range(0, len(body), MAX_TABLE_ROWS_PER_CHUNK)]
            for k, bch in enumerate(body_chunks, start=1):
                chunk_mat = header + bch
                content = tsv_table(chunk_mat)
                start_row = 1 + 2 + (k-1)*MAX_TABLE_ROWS_PER_CHUNK
                end_row   = 2 + k*MAX_TABLE_ROWS_PER_CHUNK
                table_snips.append({
                    "snippet_id": f"pdf:{source_file}:p{t['page']}:table{t['table_index']}:chunk{k}",
                    "company": company,
                    "client": client,
                    "source_file": source_file,
                    "type": "table",
                    "loc": {"page": t["page"], "table": t["table_index"], "chunk": k, "row_range": [start_row, end_row]},
                    "content": content
                })

    return stable_sort_snippets(text_snips), stable_sort_snippets(table_snips)

def build_docx_snippets(company: str, client: str, path: Path) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    if docx is None:
        raise ImportError("python-docx not installed. Run: pip install python-docx")

    d = docx.Document(str(path))
    source_file = path.name

    paras = [p.text.strip() for p in d.paragraphs if (p.text or "").strip()]
    text_snips = []
    for i, ch in enumerate(chunk_list(paras, DOCX_PARAS_PER_SNIPPET), start=1):
        content = "\n".join(ch).strip()
        if content:
            text_snips.append({
                "snippet_id": f"docx:{source_file}:text{i}",
                "company": company,
                "client": client,
                "source_file": source_file,
                "type": "text",
                "loc": {"para_chunk": i},
                "content": content
            })

    table_snips = []
    for t_i, tbl in enumerate(d.tables, start=1):
        rows = []
        for r in tbl.rows:
            rows.append([c.text.strip() for c in r.cells])
        if not rows:
            continue

        if len(rows) <= MAX_TABLE_ROWS_PER_CHUNK:
            content = tsv_table(rows)
            table_snips.append({
                "snippet_id": f"docx:{source_file}:table{t_i}",
                "company": company,
                "client": client,
                "source_file": source_file,
                "type": "table",
                "loc": {"table": t_i, "row_range": [1, len(rows)]},
                "content": content
            })
        else:
            header = rows[:2]
            body = rows[2:]
            body_chunks = [body[i:i+MAX_TABLE_ROWS_PER_CHUNK] for i in range(0, len(body), MAX_TABLE_ROWS_PER_CHUNK)]
            for k, bch in enumerate(body_chunks, start=1):
                content = tsv_table(header + bch)
                start_row = 1 + 2 + (k-1)*MAX_TABLE_ROWS_PER_CHUNK
                end_row   = 2 + k*MAX_TABLE_ROWS_PER_CHUNK
                table_snips.append({
                    "snippet_id": f"docx:{source_file}:table{t_i}:chunk{k}",
                    "company": company,
                    "client": client,
                    "source_file": source_file,
                    "type": "table",
                    "loc": {"table": t_i, "chunk": k, "row_range": [start_row, end_row]},
                    "content": content
                })

    return stable_sort_snippets(text_snips), stable_sort_snippets(table_snips)

def build_xlsx_snippets(company: str, client: str, path: Path) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    if openpyxl is None:
        raise ImportError("openpyxl not installed. Run: pip install openpyxl")

    wb = openpyxl.load_workbook(str(path), data_only=True)
    source_file = path.name

    # XLSX usually table-ish; no text snippets by default
    text_snips = []
    table_snips = []

    for sname in wb.sheetnames:
        sh = wb[sname]

        # find used range
        max_r, max_c = sh.max_row, sh.max_column
        # tighten used range by scanning for non-empty cells
        last_r, last_c = 0, 0
        for r in range(1, max_r + 1):
            row_has = False
            for c in range(1, max_c + 1):
                v = sh.cell(row=r, column=c).value
                if v is not None and str(v).strip() != "":
                    row_has = True
                    last_c = max(last_c, c)
            if row_has:
                last_r = r

        if last_r == 0 or last_c == 0:
            continue

        rows = []
        for r in range(1, last_r + 1):
            row = []
            for c in range(1, last_c + 1):
                v = sh.cell(row=r, column=c).value
                row.append("" if v is None else str(v))
            rows.append(row)

        # big sheets -> chunk into big table-block chunks
        if len(rows) <= MAX_TABLE_ROWS_PER_CHUNK:
            content = tsv_table(rows)
            table_snips.append({
                "snippet_id": f"xlsx:{source_file}:{sname}",
                "company": company,
                "client": client,
                "source_file": source_file,
                "type": "table",
                "loc": {"sheet": sname, "row_range": [1, len(rows)]},
                "content": content
            })
        else:
            header = rows[:2]
            body = rows[2:]
            body_chunks = [body[i:i+MAX_TABLE_ROWS_PER_CHUNK] for i in range(0, len(body), MAX_TABLE_ROWS_PER_CHUNK)]
            for k, bch in enumerate(body_chunks, start=1):
                content = tsv_table(header + bch)
                start_row = 1 + 2 + (k-1)*MAX_TABLE_ROWS_PER_CHUNK
                end_row   = 2 + k*MAX_TABLE_ROWS_PER_CHUNK
                table_snips.append({
                    "snippet_id": f"xlsx:{source_file}:{sname}:chunk{k}",
                    "company": company,
                    "client": client,
                    "source_file": source_file,
                    "type": "table",
                    "loc": {"sheet": sname, "chunk": k, "row_range": [start_row, end_row]},
                    "content": content
                })

    return stable_sort_snippets(text_snips), stable_sort_snippets(table_snips)


# =========================
# Bedrock Claude call
# =========================
def bedrock_claude_text(prompt: str, max_tokens: int = 1400) -> str:
    brt = boto3.client("bedrock-runtime", region_name=AWS_REGION)

    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "temperature": TEMPERATURE,
        "max_tokens": max_tokens,
        "messages": [
            {"role": "user", "content": [{"type": "text", "text": prompt}]}
        ],
    }

    resp = brt.invoke_model(
        modelId=BEDROCK_MODEL_ID,
        body=json.dumps(body).encode("utf-8"),
        contentType="application/json",
        accept="application/json",
    )
    raw = resp["body"].read().decode("utf-8")
    data = json.loads(raw)

    # Anthropic response shape: {"content":[{"type":"text","text":"..."}], ...}
    if isinstance(data, dict) and "content" in data and data["content"]:
        return "".join([c.get("text","") for c in data["content"] if c.get("type") == "text"]).strip()

    # Fallback (rare)
    return raw.strip()


# =========================
# Prompts + validation
# =========================
def build_prompt(fields: List[str], snippets: List[Dict[str, Any]], phase_label: str) -> str:
    # Keep prompt deterministic with stable ordering
    snippets = stable_sort_snippets(snippets)

    evidence_blocks = []
    for sn in snippets:
        evidence_blocks.append(
            f"SNIPPET_ID: {sn['snippet_id']}\n"
            f"SOURCE: {sn['source_file']}\n"
            f"TYPE: {sn['type']}\n"
            f"LOC: {json.dumps(sn['loc'], ensure_ascii=False)}\n"
            f"CONTENT:\n{sn['content']}\n"
        )

    fields_json = json.dumps(fields, ensure_ascii=False, indent=2)

    return f"""You are extracting structured fields for one client from provided evidence snippets.

PHASE: {phase_label}

RULES (STRICT):
- Use ONLY the provided snippets as evidence.
- If a field is not explicitly supported by the snippets, output null for that field.
- Output MUST be valid JSON ONLY. No extra text.
- For each field, return an object: {{"value": <string|null>, "snippet_id": <string|null>}}
- snippet_id must be one of the provided SNIPPET_ID values when value is not null.

FIELDS:
{fields_json}

EVIDENCE SNIPPETS:
{'\n---\n'.join(evidence_blocks)}
"""

def validate_and_prune(result: Dict[str, Any], snippet_map: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
    cleaned = {}
    for field, obj in result.items():
        if not isinstance(obj, dict):
            cleaned[field] = {"value": None, "snippet_id": None}
            continue

        value = obj.get("value", None)
        sid   = obj.get("snippet_id", None)

        if value is None or sid is None:
            cleaned[field] = {"value": None, "snippet_id": None}
            continue

        if sid not in snippet_map:
            cleaned[field] = {"value": None, "snippet_id": None}
            continue

        v = str(value).strip()
        if not v:
            cleaned[field] = {"value": None, "snippet_id": None}
            continue

        content = snippet_map[sid]["content"]
        if norm(v) in norm(content):
            cleaned[field] = {"value": v, "snippet_id": sid}
        else:
            # strict anti-hallucination: blank if not literally present
            cleaned[field] = {"value": None, "snippet_id": None}

    # Ensure all requested fields exist
    for f in FIELDS:
        cleaned.setdefault(f, {"value": None, "snippet_id": None})

    return cleaned


# =========================
# Retrieval (deterministic keyword scoring)
# =========================
def select_top_snippets_for_fields(fields: List[str], snippets: List[Dict[str, Any]], k_per_field: int,
                                  max_total: int) -> List[Dict[str, Any]]:
    # Precompute stable order for tie-breaks
    snippets = stable_sort_snippets(snippets)

    chosen_ids = []
    chosen_set = set()

    for f in fields:
        queries = [f] + FIELD_SYNONYMS.get(f, [])
        scored = []
        for sn in snippets:
            sc = score_snippet(f, queries, sn)
            if sc > 0:
                scored.append((sc, sn["snippet_id"]))

        scored.sort(key=lambda x: (-x[0], x[1]))  # deterministic
        for _, sid in scored[:k_per_field]:
            if sid not in chosen_set:
                chosen_set.add(sid)
                chosen_ids.append(sid)

        if len(chosen_ids) >= max_total:
            break

    sid2sn = {sn["snippet_id"]: sn for sn in snippets}
    return [sid2sn[sid] for sid in chosen_ids if sid in sid2sn][:max_total]


# =========================
# Main runner for ONE company/client
# =========================
def run_single_company_client(company: str, client: str) -> Dict[str, Any]:
    base_dir = DATA_ROOT / company / client
    if not base_dir.exists():
        raise FileNotFoundError(f"Local folder not found: {base_dir.resolve()}")

    files = sorted([p for p in base_dir.rglob("*") if p.is_file() and p.suffix.lower() in (".pdf", ".docx", ".xlsx")])
    if not files:
        raise FileNotFoundError(f"No pdf/docx/xlsx found under {base_dir.resolve()}")

    # Collect snippets
    all_text_snips = []
    all_table_snips = []

    textract = boto3.client("textract", region_name=AWS_REGION) if S3_BUCKET else None

    for p in files:
        ext = p.suffix.lower()
        if ext == ".pdf":
            source_file = p.name

            # Prefer Textract for PDFs (text + tables) to handle scanned PDFs reliably
            if not S3_BUCKET:
                # fallback: local PDF text only (no tables)
                if PdfReader is None:
                    raise ImportError("pypdf not installed. Run: pip install pypdf")
                reader = PdfReader(str(p))
                lines = []
                for i, page in enumerate(reader.pages, start=1):
                    t = (page.extract_text() or "").strip()
                    if t:
                        for ln in t.splitlines():
                            if ln.strip():
                                lines.append((i, ln.strip()))
                # build text snippets from extracted text; no table snippets
                lines_by_page = {}
                for pg, ln in lines:
                    lines_by_page.setdefault(pg, []).append(ln)
                for pg in sorted(lines_by_page.keys()):
                    chunks = chunk_list(lines_by_page[pg], PDF_TEXT_LINES_PER_SNIPPET)
                    for j, ch in enumerate(chunks, start=1):
                        content = "\n".join(ch).strip()
                        if content:
                            all_text_snips.append({
                                "snippet_id": f"pdf:{source_file}:p{pg}:text{j}",
                                "company": company, "client": client, "source_file": source_file,
                                "type": "text", "loc": {"page": pg, "chunk": j}, "content": content
                            })
                continue

            # Textract path: assume the PDF is in S3 at: <S3_PREFIX>/<company>/<client>/<filename>
            s3_key = f"{S3_PREFIX.rstrip('/')}/{company}/{client}/{source_file}"
            tex_json = textract_start_and_fetch_all(textract, S3_BUCKET, s3_key)
            t_sn, tab_sn = build_pdf_snippets_from_textract(company, client, source_file, tex_json)
            all_text_snips.extend(t_sn)
            all_table_snips.extend(tab_sn)

        elif ext == ".docx":
            t_sn, tab_sn = build_docx_snippets(company, client, p)
            all_text_snips.extend(t_sn)
            all_table_snips.extend(tab_sn)

        elif ext == ".xlsx":
            t_sn, tab_sn = build_xlsx_snippets(company, client, p)
            all_text_snips.extend(t_sn)
            all_table_snips.extend(tab_sn)

    all_text_snips  = stable_sort_snippets(all_text_snips)
    all_table_snips = stable_sort_snippets(all_table_snips)

    print(f"Collected snippets for {company}/{client}: text={len(all_text_snips)}, tables={len(all_table_snips)}")

    # -----------------------
    # PASS 1: TEXT-FIRST
    # -----------------------
    text_candidates = select_top_snippets_for_fields(
        fields=FIELDS,
        snippets=all_text_snips,
        k_per_field=K_TEXT_PER_FIELD,
        max_total=MAX_TEXT_SNIPPETS_TOTAL
    )
    text_map = {sn["snippet_id"]: sn for sn in text_candidates}

    prompt1 = build_prompt(FIELDS, text_candidates, phase_label="PASS_1_TEXT_ONLY")
    raw1 = bedrock_claude_text(prompt1, max_tokens=1700)
    res1 = safe_json_load(raw1)

    if not isinstance(res1, dict):
        raise ValueError("Model did not return a JSON object in PASS 1.")

    pruned1 = validate_and_prune(res1, text_map)

    # Determine missing fields
    missing = [f for f in FIELDS if not pruned1.get(f, {}).get("value")]
    print(f"PASS 1 done. Found {len(FIELDS) - len(missing)}/{len(FIELDS)} from TEXT. Missing={len(missing)}")

    # -----------------------
    # PASS 2: TABLE fallback for missing only
    # -----------------------
    final = dict(pruned1)

    if missing and all_table_snips:
        table_candidates = select_top_snippets_for_fields(
            fields=missing,
            snippets=all_table_snips,
            k_per_field=K_TABLE_PER_FIELD,
            max_total=MAX_TABLE_SNIPPETS_TOTAL
        )
        table_map = {sn["snippet_id"]: sn for sn in table_candidates}

        prompt2 = build_prompt(missing, table_candidates, phase_label="PASS_2_TABLE_ONLY_FOR_MISSING_FIELDS")
        raw2 = bedrock_claude_text(prompt2, max_tokens=1700)
        res2 = safe_json_load(raw2)

        if not isinstance(res2, dict):
            raise ValueError("Model did not return a JSON object in PASS 2.")

        pruned2 = validate_and_prune(res2, table_map)

        # merge: fill blanks only
        for f in missing:
            if pruned2.get(f, {}).get("value"):
                final[f] = pruned2[f]

        still_missing = [f for f in FIELDS if not final.get(f, {}).get("value")]
        print(f"PASS 2 done. Now missing={len(still_missing)}")

    # Output nice summary
    print("\n=== FINAL ANSWERS ===")
    for f in FIELDS:
        v = final.get(f, {}).get("value")
        sid = final.get(f, {}).get("snippet_id")
        print(f"- {f}: {v if v else ''}  [{sid if sid else ''}]")

    return final


# =========================
# RUN
# =========================
results = run_single_company_client(COMPANY, CLIENT)
