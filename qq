# ONE-CELL deterministic debug (improved):
# PASS 1: TEXT-only (avoid tables)
# PASS 2: TABLE-only using TABLE_BLOCKS (collapse TABLE_ROW into whole table chunks)
# Still uses your existing evidence.jsonl + faiss.index (no re-indexing)

import json, os, re, time
from pathlib import Path
from collections import defaultdict

import faiss
import numpy as np
import boto3


# ----------------- load config/evidence/faiss -----------------
cfg = json.loads(Path("config.json").read_text(encoding="utf-8"))
AWS_REGION = cfg.get("aws_region", "us-east-1")

evidence_path = Path(cfg["evidence_jsonl_path"])
faiss_path = cfg["faiss_index_path"]

evidence = []
with open(evidence_path, "r", encoding="utf-8") as f:
    for line in f:
        evidence.append(json.loads(line))

index = faiss.read_index(faiss_path)

print("[dbg] evidence rows:", len(evidence))
print("[dbg] faiss ntotal:", index.ntotal)
print("[dbg] sample keys:", list(evidence[0].keys()) if evidence else None)


# ----------------- bedrock clients -----------------
BEDROCK_EMBED_MODEL_ID = os.getenv("BEDROCK_EMBED_MODEL_ID")
BEDROCK_LLM_MODEL_ID = os.getenv("BEDROCK_LLM_MODEL_ID")

print("[dbg] AWS_REGION:", AWS_REGION)
print("[dbg] EMBED_MODEL:", BEDROCK_EMBED_MODEL_ID)
print("[dbg] LLM_MODEL:", BEDROCK_LLM_MODEL_ID)

if not BEDROCK_EMBED_MODEL_ID or not BEDROCK_LLM_MODEL_ID:
    raise RuntimeError("Set env vars BEDROCK_EMBED_MODEL_ID and BEDROCK_LLM_MODEL_ID")

session = boto3.Session(region_name=AWS_REGION)
bedrock = session.client("bedrock-runtime")


# ----------------- knobs (safe defaults) -----------------
# If you truly want "one complete table in one chunk", keep TABLE_BLOCK_MAX_ROWS=None.
# If you hit Claude context overflow, set e.g. TABLE_BLOCK_MAX_ROWS=300 or 500 (still block-based, not row-based).
TABLE_BLOCK_MAX_ROWS = None     # None => keep full table; else chunk table blocks by row count
TABLE_BLOCK_MAX_CHARS = 25000   # safety guard: if table text > this, chunk by rows even if max_rows is None

# Evidence pack sizing (from config)
MAX_UNITS = int(cfg.get("max_evidence_units", 90))
MAX_TEXT  = int(cfg.get("max_text_units", 30))
MAX_TABLE = int(cfg.get("max_table_units", 60))

# Retrieval sizing (keep your current)
TOP_EACH = 25
TOP_GLOBAL = 250

# ----------------- helpers -----------------
def embed_titan(text: str) -> np.ndarray:
    resp = bedrock.invoke_model(
        modelId=BEDROCK_EMBED_MODEL_ID,
        body=json.dumps({"inputText": text}),
        accept="application/json",
        contentType="application/json",
    )
    vec = json.loads(resp["body"].read())["embedding"]
    v = np.array(vec, dtype="float32")
    n = np.linalg.norm(v)
    if n > 0:
        v = v / n
    return v.reshape(1, -1)

def extract_text_any(raw: dict) -> str:
    if isinstance(raw, dict):
        if "content" in raw and isinstance(raw["content"], list) and raw["content"]:
            first = raw["content"][0]
            if isinstance(first, dict) and "text" in first:
                return first["text"]
        out = raw.get("output", {})
        msg = out.get("message", {})
        cont = msg.get("content", [])
        if isinstance(cont, list) and cont and isinstance(cont[0], dict) and "text" in cont[0]:
            return cont[0]["text"]
        if "completion" in raw and isinstance(raw["completion"], str):
            return raw["completion"]
    return ""

def call_claude(prompt: str) -> str:
    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "temperature": 0,
        "max_tokens": 1400,
        "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}],
    }
    resp = bedrock.invoke_model(
        modelId=BEDROCK_LLM_MODEL_ID,
        body=json.dumps(body),
        accept="application/json",
        contentType="application/json",
    )
    raw_text = resp["body"].read().decode("utf-8", errors="replace")
    try:
        raw_json = json.loads(raw_text)
    except Exception:
        print("\n[dbg] Bedrock raw body (first 800 chars):")
        print(raw_text[:800])
        return ""
    return extract_text_any(raw_json)

def normalize(s: str) -> str:
    s = (s or "").replace("\u00a0", " ")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def prune_fields(fields_dict, evidence_text):
    ev = normalize(evidence_text)
    cleaned = {}
    for k, v in (fields_dict or {}).items():
        if isinstance(v, str) and v.strip():
            if normalize(v) in ev:
                cleaned[k] = v
    return cleaned

# IMPORTANT CHANGE: text-first ranking
def stable_sort_key(it):
    # Prefer TEXT evidence first; tables later
    type_rank = 0 if it.get("type") != "TABLE_ROW" and it.get("type") != "TABLE_BLOCK" else 1
    file_name = it.get("file", "")
    loc = it.get("loc", "")
    table_id = it.get("table_id") or ""
    row = it.get("row_index") if it.get("row_index") is not None else 10**9
    # also allow score (higher better) for deterministic rerank
    score = it.get("_score", 0.0)
    return (-float(score), type_rank, file_name, loc, table_id, row, it.get("doc_id", 0))

def pick_evidence(items, max_units=80, max_table=50, max_text=30, allow_tables=True):
    items = list(items)
    items.sort(key=stable_sort_key)
    picked, t_count, x_count = [], 0, 0
    for it in items:
        is_table = it.get("type") in ("TABLE_ROW", "TABLE_BLOCK")
        if is_table:
            if not allow_tables:
                continue
            if t_count >= max_table:
                continue
            t_count += 1
        else:
            if x_count >= max_text:
                continue
            x_count += 1
        picked.append(it)
        if len(picked) >= max_units:
            break
    return picked

# ----------------- NEW: keep scores from FAISS searches -----------------
def retrieve_union_for_queries(company, client, queries, top_each=25, top_global=250):
    """
    Returns candidate evidence items with a stored _score (best similarity seen for that doc_id).
    This improves evidence picking (still deterministic).
    """
    best_score = {}  # doc_id -> best score (higher better for IP/cosine)
    all_ids = set()

    def _add(doc_id: int, score: float):
        if doc_id < 0:
            return
        it = evidence[doc_id]
        if it.get("company") != company or it.get("client_id") != client:
            return
        all_ids.add(doc_id)
        prev = best_score.get(doc_id)
        if prev is None or float(score) > float(prev):
            best_score[doc_id] = float(score)

    # per-query
    for q in queries:
        qvec = embed_titan(q)
        scores, ids = index.search(qvec, top_each)
        for s, doc_id in zip(scores[0].tolist(), ids[0].tolist()):
            _add(doc_id, s)

    # global fallback
    qvec = embed_titan(" | ".join(queries))
    scores, ids = index.search(qvec, top_global)
    for s, doc_id in zip(scores[0].tolist(), ids[0].tolist()):
        _add(doc_id, s)

    items = []
    for i in sorted(all_ids):
        it = dict(evidence[i])
        it["doc_id"] = i
        it["_score"] = best_score.get(i, 0.0)
        items.append(it)
    return items

# ----------------- NEW: collapse TABLE_ROW -> TABLE_BLOCK (whole table as one chunk) -----------------
def _parse_row_content(row_text: str):
    """
    Tries to extract header/value lines from your TABLE_ROW content.
    Works with templates like:
      HEADERS: ...
      VALUES: ...
    Falls back to raw row_text.
    """
    headers = None
    values = None
    lines = [ln.strip() for ln in (row_text or "").splitlines() if ln.strip()]
    for ln in lines:
        if ln.upper().startswith("HEADERS:"):
            headers = ln.split(":", 1)[1].strip()
        elif ln.upper().startswith("VALUES:"):
            values = ln.split(":", 1)[1].strip()
    return headers, values, row_text.strip()

def collapse_table_rows_to_blocks(items):
    """
    Convert many TABLE_ROW items into TABLE_BLOCK items.
    Group key: (file, table_id, loc_prefix)
    Deterministic: rows sorted by row_index, score=max row score.
    """
    # keep non-table as-is
    non_table = [it for it in items if it.get("type") not in ("TABLE_ROW", "TABLE_BLOCK")]
    table_rows = [it for it in items if it.get("type") == "TABLE_ROW"]

    groups = defaultdict(list)

    def table_group_key(it):
        # Use table_id if present; else derive from loc.
        file = it.get("file", "")
        table_id = it.get("table_id") or ""
        loc = it.get("loc", "") or ""
        # loc_prefix reduces accidental merge across different tables if table_id missing
        loc_prefix = loc.split(" ROW=")[0].strip() if isinstance(loc, str) else str(loc)
        return (file, table_id, loc_prefix)

    for r in table_rows:
        groups[table_group_key(r)].append(r)

    table_blocks = []
    for (file, table_id, loc_prefix), rows in sorted(groups.items(), key=lambda x: (x[0][0], x[0][1], x[0][2])):
        # sort rows deterministically
        rows.sort(key=lambda x: (x.get("row_index") if x.get("row_index") is not None else 10**9, x.get("doc_id", 0)))

        # build block text
        header_line = None
        value_lines = []
        max_score = 0.0
        doc_ids = []

        for r in rows:
            doc_ids.append(r.get("doc_id"))
            max_score = max(max_score, float(r.get("_score", 0.0)))
            h, v, raw = _parse_row_content(r.get("content", ""))
            if header_line is None and h:
                header_line = h
            if v:
                value_lines.append(v)
            else:
                # fallback: include the raw row content
                value_lines.append(raw)

        # If you want "one complete table block", this will include all rows.
        # But if it's too large, we chunk by row count/char count (still TABLE_BLOCK, not TABLE_ROW).
        def _make_block(chunk_rows, chunk_idx=1):
            rs = chunk_rows
            r0 = rs[0].get("row_index")
            r1 = rs[-1].get("row_index")
            hdr = header_line or "(unknown headers)"
            vals = []
            for rr in rs:
                _, v, raw = _parse_row_content(rr.get("content", ""))
                vals.append(v if v else raw)
            content = (
                f"TYPE=TABLE_BLOCK\n"
                f"FILE={file}\n"
                f"TABLE_ID={table_id}\n"
                f"LOC={loc_prefix}\n"
                f"ROW_RANGE={r0}-{r1}\n"
                f"HEADERS: {hdr}\n"
                f"ROWS:\n- " + "\n- ".join(vals)
            )
            block = {
                "type": "TABLE_BLOCK",
                "file": file,
                "table_id": table_id,
                "loc": loc_prefix,
                "row_index": r0 if r0 is not None else 0,
                "doc_id": min([d for d in doc_ids if isinstance(d, int)], default=10**9),
                "_score": max_score,
                "content": content,
                "_source_doc_ids": doc_ids,
            }
            if chunk_idx > 1:
                block["loc"] = f"{loc_prefix} CHUNK={chunk_idx}"
            return block

        # chunk policy
        blocks_for_this_table = []
        if TABLE_BLOCK_MAX_ROWS is None:
            # keep full, but still split if content too large
            tmp_block = _make_block(rows, 1)
            if len(tmp_block["content"]) <= TABLE_BLOCK_MAX_CHARS:
                blocks_for_this_table = [tmp_block]
            else:
                # split by rows to stay within safe chars; deterministic
                chunk_size = 300
                chunk_idx = 1
                for i in range(0, len(rows), chunk_size):
                    blocks_for_this_table.append(_make_block(rows[i:i+chunk_size], chunk_idx))
                    chunk_idx += 1
        else:
            chunk_idx = 1
            for i in range(0, len(rows), int(TABLE_BLOCK_MAX_ROWS)):
                blocks_for_this_table.append(_make_block(rows[i:i+int(TABLE_BLOCK_MAX_ROWS)], chunk_idx))
                chunk_idx += 1

        table_blocks.extend(blocks_for_this_table)

    # Return combined list
    combined = non_table + table_blocks
    combined.sort(key=stable_sort_key)
    return combined


# ----------------- choose company/client -----------------
pairs = sorted(set((x.get("company"), x.get("client_id")) for x in evidence if x.get("company") and x.get("client_id")))
print("\n[dbg] pairs:", len(pairs))
print("[dbg] first 10:", pairs[:10])

TEST_COMPANY, TEST_CLIENT = pairs[0]  # <-- change if needed
print("\n[dbg] TESTING:", TEST_COMPANY, TEST_CLIENT)


# ----------------- fields (+ synonyms for retrieval only) -----------------
FIELDS = [

]

FIELD_SYNONYMS = {

}

def expand_queries(fields):
    qs = []
    for f in fields:
        qs.append(f)
        for syn in FIELD_SYNONYMS.get(f, []):
            qs.append(syn)
    return qs


# ----------------- PASS 1: Retrieve + pick TEXT ONLY -----------------
t0 = time.time()
queries1 = expand_queries(FIELDS)
cand1 = retrieve_union_for_queries(TEST_COMPANY, TEST_CLIENT, queries1, top_each=TOP_EACH, top_global=TOP_GLOBAL)

# remove tables completely in pass 1
cand1_text_only = [it for it in cand1 if it.get("type") != "TABLE_ROW" and it.get("type") != "TABLE_BLOCK"]

picked1 = pick_evidence(
    cand1_text_only,
    max_units=MAX_UNITS,
    max_table=0,
    max_text=max(MAX_TEXT, MAX_UNITS),  # allow mostly text
    allow_tables=False
)

print(f"\n[dbg] PASS 1 candidates (text-only): {len(cand1_text_only)} | picked: {len(picked1)} | took {time.time()-t0:.2f}s")

evidence_blob_1 = "\n\n".join([x.get("content", "") for x in picked1])

prompt1 = (
    "You are a strict extraction engine.\n\n"
    "Rules:\n"
    "1) Use ONLY the provided evidence.\n"
    "2) Return ONLY a valid JSON object. No markdown. No commentary.\n"
    "3) ONLY include fields where the VALUE can be copied verbatim from evidence.\n"
    "4) If not found, OMIT the field.\n"
    "5) Do not infer, normalize, or compute.\n\n"
    "Output format:\n"
    "{\n"
    f'  "Company": "{TEST_COMPANY}",\n'
    f'  "Client": "{TEST_CLIENT}",\n'
    '  "fields": {\n'
    '    "<Field Name>": "<verbatim value>"\n'
    "  }\n"
    "}\n\n"
    "Fields:\n- " + "\n- ".join(FIELDS) + "\n\n"
    "EVIDENCE:\n" + evidence_blob_1
)

print("\n[dbg] PASS 1 calling Claude (TEXT ONLY)...")
out1 = call_claude(prompt1)
print("\n[dbg] PASS 1 Claude output (first 1200 chars):\n", (out1 or "")[:1200])

if not out1:
    raise RuntimeError("PASS 1: Claude returned empty output")

parsed1 = json.loads(out1)
raw_fields_1 = parsed1.get("fields", {}) if isinstance(parsed1, dict) else {}
pruned1 = prune_fields(raw_fields_1, evidence_blob_1)

missing = [f for f in FIELDS if not pruned1.get(f)]
print("\n[dbg] PASS 1 pruned fields:", list(pruned1.keys()))
print("[dbg] PASS 1 missing fields:", missing)


# ----------------- PASS 2: Retrieve + collapse TABLE_ROW -> TABLE_BLOCK for missing only -----------------
final_fields = dict(pruned1)

if missing:
    t0 = time.time()
    queries2 = expand_queries(missing)
    cand2 = retrieve_union_for_queries(TEST_COMPANY, TEST_CLIENT, queries2, top_each=TOP_EACH, top_global=TOP_GLOBAL)

    # collapse table rows into whole-table blocks
    cand2_collapsed = collapse_table_rows_to_blocks(cand2)

    # keep ONLY table blocks in pass 2 (tables later)
    cand2_table_only = [it for it in cand2_collapsed if it.get("type") == "TABLE_BLOCK"]

    picked2 = pick_evidence(
        cand2_table_only,
        max_units=MAX_UNITS,
        max_table=MAX_TABLE,
        max_text=0,
        allow_tables=True
    )

    print(f"\n[dbg] PASS 2 candidates (table-blocks): {len(cand2_table_only)} | picked: {len(picked2)} | took {time.time()-t0:.2f}s")

    evidence_blob_2 = "\n\n".join([x.get("content", "") for x in picked2])

    prompt2 = (
        "You are a strict extraction engine.\n\n"
        "Rules:\n"
        "1) Use ONLY the provided evidence.\n"
        "2) Return ONLY a valid JSON object. No markdown. No commentary.\n"
        "3) ONLY include fields where the VALUE can be copied verbatim from evidence.\n"
        "4) If not found, OMIT the field.\n"
        "5) Do not infer, normalize, or compute.\n\n"
        "Output format:\n"
        "{\n"
        f'  "Company": "{TEST_COMPANY}",\n'
        f'  "Client": "{TEST_CLIENT}",\n'
        '  "fields": {\n'
        '    "<Field Name>": "<verbatim value>"\n'
        "  }\n"
        "}\n\n"
        "Fields (ONLY THESE are missing from text):\n- " + "\n- ".join(missing) + "\n\n"
        "EVIDENCE:\n" + evidence_blob_2
    )

    print("\n[dbg] PASS 2 calling Claude (TABLE_BLOCKS ONLY)...")
    out2 = call_claude(prompt2)
    print("\n[dbg] PASS 2 Claude output (first 1200 chars):\n", (out2 or "")[:1200])

    if out2:
        parsed2 = json.loads(out2)
        raw_fields_2 = parsed2.get("fields", {}) if isinstance(parsed2, dict) else {}
        pruned2 = prune_fields(raw_fields_2, evidence_blob_2)

        # fill only missing
        for f in missing:
            if pruned2.get(f):
                final_fields[f] = pruned2[f]

# ----------------- FINAL ROW (blank if missing) -----------------
final_row = {"Company": TEST_COMPANY, "Client": TEST_CLIENT}
for f in FIELDS:
    final_row[f] = final_fields.get(f, "")

print("\n========== FINAL ROW OUTPUT ==========")
print("Company:", final_row["Company"])
print("Client:", final_row["Client"])
print("\nFields:")
for f in FIELDS:
    if final_row[f]:
        print(f"- {f}: {final_row[f]}")
    else:
        print(f"- {f}: (blank)")
